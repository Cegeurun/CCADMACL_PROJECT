{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "b1628b2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries imported successfully!\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans, DBSCAN, AgglomerativeClustering\n",
    "from sklearn.metrics import silhouette_score, davies_bouldin_score, calinski_harabasz_score\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage\n",
    "from scipy.spatial.distance import cdist\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set plotting style\n",
    "sns.set_style(\"whitegrid\")\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdcf2ec",
   "metadata": {},
   "source": [
    "# Video Game Clustering Project\n",
    "## Unsupervised Learning Framework for Game Archetype Segmentation\n",
    "\n",
    "### Project Goal\n",
    "Automatically segment video games into distinct archetypes (e.g., \"Critically Acclaimed Blockbusters\", \"Underrated Gems\") by analyzing patterns between commercial performance and critical reception.\n",
    "\n",
    "### Dataset\n",
    "Kaggle Video Game Sales with Ratings: https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings/data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e41c647",
   "metadata": {},
   "source": [
    "## Step 1: Data Loading and Initial Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3feab8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîµ Running in Google Colab!\n",
      "Please upload 'Video_Games_Sales_as_at_22_Dec_2016.csv' from your computer...\n",
      "(It should be in your local data folder)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "     <input type=\"file\" id=\"files-1098f8be-3931-4a1e-acd4-4e1a1f570034\" name=\"files[]\" multiple disabled\n",
       "        style=\"border:none\" />\n",
       "     <output id=\"result-1098f8be-3931-4a1e-acd4-4e1a1f570034\">\n",
       "      Upload widget is only available when the cell has been executed in the\n",
       "      current browser session. Please rerun this cell to enable.\n",
       "      </output>\n",
       "      <script>// Copyright 2017 Google LLC\n",
       "//\n",
       "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
       "// you may not use this file except in compliance with the License.\n",
       "// You may obtain a copy of the License at\n",
       "//\n",
       "//      http://www.apache.org/licenses/LICENSE-2.0\n",
       "//\n",
       "// Unless required by applicable law or agreed to in writing, software\n",
       "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
       "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
       "// See the License for the specific language governing permissions and\n",
       "// limitations under the License.\n",
       "\n",
       "/**\n",
       " * @fileoverview Helpers for google.colab Python module.\n",
       " */\n",
       "(function(scope) {\n",
       "function span(text, styleAttributes = {}) {\n",
       "  const element = document.createElement('span');\n",
       "  element.textContent = text;\n",
       "  for (const key of Object.keys(styleAttributes)) {\n",
       "    element.style[key] = styleAttributes[key];\n",
       "  }\n",
       "  return element;\n",
       "}\n",
       "\n",
       "// Max number of bytes which will be uploaded at a time.\n",
       "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
       "\n",
       "function _uploadFiles(inputId, outputId) {\n",
       "  const steps = uploadFilesStep(inputId, outputId);\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  // Cache steps on the outputElement to make it available for the next call\n",
       "  // to uploadFilesContinue from Python.\n",
       "  outputElement.steps = steps;\n",
       "\n",
       "  return _uploadFilesContinue(outputId);\n",
       "}\n",
       "\n",
       "// This is roughly an async generator (not supported in the browser yet),\n",
       "// where there are multiple asynchronous steps and the Python side is going\n",
       "// to poll for completion of each step.\n",
       "// This uses a Promise to block the python side on completion of each step,\n",
       "// then passes the result of the previous step as the input to the next step.\n",
       "function _uploadFilesContinue(outputId) {\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  const steps = outputElement.steps;\n",
       "\n",
       "  const next = steps.next(outputElement.lastPromiseValue);\n",
       "  return Promise.resolve(next.value.promise).then((value) => {\n",
       "    // Cache the last promise value to make it available to the next\n",
       "    // step of the generator.\n",
       "    outputElement.lastPromiseValue = value;\n",
       "    return next.value.response;\n",
       "  });\n",
       "}\n",
       "\n",
       "/**\n",
       " * Generator function which is called between each async step of the upload\n",
       " * process.\n",
       " * @param {string} inputId Element ID of the input file picker element.\n",
       " * @param {string} outputId Element ID of the output display.\n",
       " * @return {!Iterable<!Object>} Iterable of next steps.\n",
       " */\n",
       "function* uploadFilesStep(inputId, outputId) {\n",
       "  const inputElement = document.getElementById(inputId);\n",
       "  inputElement.disabled = false;\n",
       "\n",
       "  const outputElement = document.getElementById(outputId);\n",
       "  outputElement.innerHTML = '';\n",
       "\n",
       "  const pickedPromise = new Promise((resolve) => {\n",
       "    inputElement.addEventListener('change', (e) => {\n",
       "      resolve(e.target.files);\n",
       "    });\n",
       "  });\n",
       "\n",
       "  const cancel = document.createElement('button');\n",
       "  inputElement.parentElement.appendChild(cancel);\n",
       "  cancel.textContent = 'Cancel upload';\n",
       "  const cancelPromise = new Promise((resolve) => {\n",
       "    cancel.onclick = () => {\n",
       "      resolve(null);\n",
       "    };\n",
       "  });\n",
       "\n",
       "  // Wait for the user to pick the files.\n",
       "  const files = yield {\n",
       "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
       "    response: {\n",
       "      action: 'starting',\n",
       "    }\n",
       "  };\n",
       "\n",
       "  cancel.remove();\n",
       "\n",
       "  // Disable the input element since further picks are not allowed.\n",
       "  inputElement.disabled = true;\n",
       "\n",
       "  if (!files) {\n",
       "    return {\n",
       "      response: {\n",
       "        action: 'complete',\n",
       "      }\n",
       "    };\n",
       "  }\n",
       "\n",
       "  for (const file of files) {\n",
       "    const li = document.createElement('li');\n",
       "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
       "    li.append(span(\n",
       "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
       "        `last modified: ${\n",
       "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
       "                                    'n/a'} - `));\n",
       "    const percent = span('0% done');\n",
       "    li.appendChild(percent);\n",
       "\n",
       "    outputElement.appendChild(li);\n",
       "\n",
       "    const fileDataPromise = new Promise((resolve) => {\n",
       "      const reader = new FileReader();\n",
       "      reader.onload = (e) => {\n",
       "        resolve(e.target.result);\n",
       "      };\n",
       "      reader.readAsArrayBuffer(file);\n",
       "    });\n",
       "    // Wait for the data to be ready.\n",
       "    let fileData = yield {\n",
       "      promise: fileDataPromise,\n",
       "      response: {\n",
       "        action: 'continue',\n",
       "      }\n",
       "    };\n",
       "\n",
       "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
       "    let position = 0;\n",
       "    do {\n",
       "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
       "      const chunk = new Uint8Array(fileData, position, length);\n",
       "      position += length;\n",
       "\n",
       "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
       "      yield {\n",
       "        response: {\n",
       "          action: 'append',\n",
       "          file: file.name,\n",
       "          data: base64,\n",
       "        },\n",
       "      };\n",
       "\n",
       "      let percentDone = fileData.byteLength === 0 ?\n",
       "          100 :\n",
       "          Math.round((position / fileData.byteLength) * 100);\n",
       "      percent.textContent = `${percentDone}% done`;\n",
       "\n",
       "    } while (position < fileData.byteLength);\n",
       "  }\n",
       "\n",
       "  // All done.\n",
       "  yield {\n",
       "    response: {\n",
       "      action: 'complete',\n",
       "    }\n",
       "  };\n",
       "}\n",
       "\n",
       "scope.google = scope.google || {};\n",
       "scope.google.colab = scope.google.colab || {};\n",
       "scope.google.colab._files = {\n",
       "  _uploadFiles,\n",
       "  _uploadFilesContinue,\n",
       "};\n",
       "})(self);\n",
       "</script> "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipython-input-283698477.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0;32mfrom\u001b[0m \u001b[0mgoogle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolab\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0muploaded\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfiles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;31m# Get the uploaded filename\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36mupload\u001b[0;34m(target_dir)\u001b[0m\n\u001b[1;32m     67\u001b[0m   \"\"\"\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m   \u001b[0muploaded_files\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_upload_files\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmultiple\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m   \u001b[0;31m# Mapping from original filename to filename as saved locally.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m   \u001b[0mlocal_filenames\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/files.py\u001b[0m in \u001b[0;36m_upload_files\u001b[0;34m(multiple)\u001b[0m\n\u001b[1;32m    159\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    160\u001b[0m   \u001b[0;31m# First result is always an indication that the file picker has completed.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 161\u001b[0;31m   result = _output.eval_js(\n\u001b[0m\u001b[1;32m    162\u001b[0m       'google.colab._files._uploadFiles(\"{input_id}\", \"{output_id}\")'.format(\n\u001b[1;32m    163\u001b[0m           \u001b[0minput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_id\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_id\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/output/_js.py\u001b[0m in \u001b[0;36meval_js\u001b[0;34m(script, ignore_result, timeout_sec)\u001b[0m\n\u001b[1;32m     38\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mignore_result\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m     \u001b[0;32mreturn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_message\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_reply_from_input\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrequest_id\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout_sec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.12/dist-packages/google/colab/_message.py\u001b[0m in \u001b[0;36mread_reply_from_input\u001b[0;34m(message_id, timeout_sec)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[0mreply\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_read_next_input_message\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mreply\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_NOT_READY\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreply\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m       \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msleep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.025\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m       \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     if (\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Load the dataset from GitHub\n",
    "# This works for both Colab and local execution - no upload needed!\n",
    "\n",
    "try:\n",
    "    print(\"üì• Loading dataset from GitHub...\")\n",
    "    \n",
    "    # Direct URL to the CSV file in your GitHub repository\n",
    "    github_url = 'https://raw.githubusercontent.com/Cegeurun/CCADMACL_PROJECT/main/data/Video_Games_Sales_as_at_22_Dec_2016.csv'\n",
    "    \n",
    "    df = pd.read_csv(github_url)\n",
    "    \n",
    "    print(f\"‚úÖ Dataset loaded successfully from GitHub!\")\n",
    "    print(f\"Shape: {df.shape}\")\n",
    "    print(f\"\\nFirst few rows:\")\n",
    "    display(df.head())\n",
    "    \n",
    "    print(f\"\\nDataset Info:\")\n",
    "    print(df.info())\n",
    "    \n",
    "    print(f\"\\nBasic Statistics:\")\n",
    "    display(df.describe())\n",
    "    \n",
    "    print(f\"\\nMissing Values:\")\n",
    "    print(df.isnull().sum())\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"‚ùå ERROR loading dataset: {str(e)}\")\n",
    "    print(\"\\nTroubleshooting:\")\n",
    "    print(\"  1. Check your internet connection\")\n",
    "    print(\"  2. Verify the GitHub repository is public\")\n",
    "    print(\"  3. Confirm the file exists at:\")\n",
    "    print(\"     https://github.com/Cegeurun/CCADMACL_PROJECT/blob/main/data/Video_Games_Sales_as_at_22_Dec_2016.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71261305",
   "metadata": {},
   "source": [
    "## Step 2: Data Cleaning and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "28c9007c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset not yet loaded. Please load the dataset first.\n"
     ]
    }
   ],
   "source": [
    "# Data cleaning function\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Clean and preprocess the video game dataset\n",
    "    Focus on sales and rating features\n",
    "    \"\"\"\n",
    "    print(\"Starting data cleaning...\")\n",
    "    print(f\"Initial shape: {df.shape}\")\n",
    "    \n",
    "    # Select relevant columns for clustering\n",
    "    # Sales columns: NA_Sales, EU_Sales, JP_Sales, Other_Sales, Global_Sales\n",
    "    # Rating columns: Critic_Score, Critic_Count, User_Score, User_Count\n",
    "    \n",
    "    relevant_cols = ['Name', 'Platform', 'Year_of_Release', 'Genre', \n",
    "                     'NA_Sales', 'EU_Sales', 'JP_Sales', 'Other_Sales', 'Global_Sales',\n",
    "                     'Critic_Score', 'Critic_Count', 'User_Score', 'User_Count']\n",
    "    \n",
    "    # Keep only relevant columns that exist\n",
    "    available_cols = [col for col in relevant_cols if col in df.columns]\n",
    "    df_clean = df[available_cols].copy()\n",
    "    \n",
    "    # Handle User_Score (sometimes stored as string with 'tbd' values)\n",
    "    if 'User_Score' in df_clean.columns:\n",
    "        df_clean['User_Score'] = pd.to_numeric(df_clean['User_Score'], errors='coerce')\n",
    "    \n",
    "    # Drop rows with missing values in key features\n",
    "    key_features = ['Global_Sales', 'Critic_Score', 'User_Score']\n",
    "    key_features = [col for col in key_features if col in df_clean.columns]\n",
    "    \n",
    "    print(f\"\\nMissing values before cleaning:\")\n",
    "    print(df_clean[key_features].isnull().sum())\n",
    "    \n",
    "    df_clean = df_clean.dropna(subset=key_features)\n",
    "    \n",
    "    # Remove duplicates\n",
    "    df_clean = df_clean.drop_duplicates()\n",
    "    \n",
    "    # Remove outliers (optional - games with extremely low engagement)\n",
    "    if 'Critic_Count' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['Critic_Count'] >= 4]\n",
    "    if 'User_Count' in df_clean.columns:\n",
    "        df_clean = df_clean[df_clean['User_Count'] >= 4]\n",
    "    \n",
    "    print(f\"\\nFinal shape after cleaning: {df_clean.shape}\")\n",
    "    print(f\"Removed {df.shape[0] - df_clean.shape[0]} rows\")\n",
    "    \n",
    "    return df_clean\n",
    "\n",
    "# Apply cleaning (will work once dataset is loaded)\n",
    "try:\n",
    "    df_clean = clean_data(df)\n",
    "    print(\"\\nCleaned dataset preview:\")\n",
    "    display(df_clean.head(10))\n",
    "    print(\"\\nCleaned dataset statistics:\")\n",
    "    display(df_clean.describe())\n",
    "except NameError:\n",
    "    print(\"Dataset not yet loaded. Please load the dataset first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15276f57",
   "metadata": {},
   "source": [
    "## Step 3: Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "814a8c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned dataset not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def engineer_features(df):\n",
    "    \"\"\"\n",
    "    Create meaningful features for clustering:\n",
    "    1. Critic-User Score Discrepancy (do critics and users agree?)\n",
    "    2. Sales-to-Quality Ratio (commercial success vs critical acclaim)\n",
    "    3. Normalized scores (scale User_Score to match Critic_Score range)\n",
    "    4. Performance categories\n",
    "    \"\"\"\n",
    "    df_features = df.copy()\n",
    "    \n",
    "    print(\"Engineering features for clustering...\")\n",
    "    \n",
    "    # Normalize User_Score (0-10) to match Critic_Score scale (0-100)\n",
    "    if 'User_Score' in df_features.columns:\n",
    "        df_features['User_Score_Normalized'] = df_features['User_Score'] * 10\n",
    "    \n",
    "    # Feature 1: Critic-User Discrepancy (positive = critics like more, negative = users like more)\n",
    "    if 'Critic_Score' in df_features.columns and 'User_Score_Normalized' in df_features.columns:\n",
    "        df_features['Critic_User_Gap'] = df_features['Critic_Score'] - df_features['User_Score_Normalized']\n",
    "    \n",
    "    # Feature 2: Average Quality Score (consensus between critics and users)\n",
    "    if 'Critic_Score' in df_features.columns and 'User_Score_Normalized' in df_features.columns:\n",
    "        df_features['Avg_Quality_Score'] = (df_features['Critic_Score'] + df_features['User_Score_Normalized']) / 2\n",
    "    \n",
    "    # Feature 3: Sales per Quality Point (commercial success relative to quality)\n",
    "    if 'Global_Sales' in df_features.columns and 'Avg_Quality_Score' in df_features.columns:\n",
    "        # Avoid division by zero\n",
    "        df_features['Sales_Quality_Ratio'] = df_features['Global_Sales'] / (df_features['Avg_Quality_Score'] + 1)\n",
    "    \n",
    "    # Feature 4: Log-transformed sales (handle skewness)\n",
    "    if 'Global_Sales' in df_features.columns:\n",
    "        df_features['Log_Global_Sales'] = np.log1p(df_features['Global_Sales'])\n",
    "    \n",
    "    # Feature 5: Engagement Index (combination of review counts)\n",
    "    if 'Critic_Count' in df_features.columns and 'User_Count' in df_features.columns:\n",
    "        df_features['Engagement_Index'] = np.log1p(df_features['Critic_Count'] + df_features['User_Count'])\n",
    "    \n",
    "    print(f\"\\nNew features created:\")\n",
    "    new_features = ['User_Score_Normalized', 'Critic_User_Gap', 'Avg_Quality_Score', \n",
    "                    'Sales_Quality_Ratio', 'Log_Global_Sales', 'Engagement_Index']\n",
    "    for feat in new_features:\n",
    "        if feat in df_features.columns:\n",
    "            print(f\"  - {feat}\")\n",
    "    \n",
    "    return df_features\n",
    "\n",
    "# Apply feature engineering\n",
    "try:\n",
    "    df_features = engineer_features(df_clean)\n",
    "    print(\"\\nFeature statistics:\")\n",
    "    feature_cols = ['Global_Sales', 'Critic_Score', 'User_Score', 'User_Score_Normalized',\n",
    "                    'Critic_User_Gap', 'Avg_Quality_Score', 'Sales_Quality_Ratio', \n",
    "                    'Log_Global_Sales', 'Engagement_Index']\n",
    "    feature_cols = [col for col in feature_cols if col in df_features.columns]\n",
    "    display(df_features[feature_cols].describe())\n",
    "except NameError:\n",
    "    print(\"Cleaned dataset not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fde12743",
   "metadata": {},
   "source": [
    "## Step 4: Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "5ae0cefb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with features not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def visualize_eda(df):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations for exploratory data analysis\n",
    "    \"\"\"\n",
    "    \n",
    "    # 1. Distribution of key features\n",
    "    fig, axes = plt.subplots(2, 3, figsize=(18, 10))\n",
    "    fig.suptitle('Distribution of Key Features', fontsize=16, fontweight='bold')\n",
    "    \n",
    "    features_to_plot = ['Global_Sales', 'Critic_Score', 'User_Score', \n",
    "                        'Critic_User_Gap', 'Log_Global_Sales', 'Avg_Quality_Score']\n",
    "    \n",
    "    for idx, feature in enumerate(features_to_plot):\n",
    "        if feature in df.columns:\n",
    "            row, col = idx // 3, idx % 3\n",
    "            axes[row, col].hist(df[feature].dropna(), bins=50, color='skyblue', edgecolor='black', alpha=0.7)\n",
    "            axes[row, col].set_title(f'{feature} Distribution')\n",
    "            axes[row, col].set_xlabel(feature)\n",
    "            axes[row, col].set_ylabel('Frequency')\n",
    "            axes[row, col].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. Correlation heatmap\n",
    "    print(\"\\nCorrelation Analysis:\")\n",
    "    numeric_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    correlation_matrix = df[numeric_cols].corr()\n",
    "    \n",
    "    plt.figure(figsize=(12, 10))\n",
    "    sns.heatmap(correlation_matrix, annot=True, fmt='.2f', cmap='coolwarm', \n",
    "                center=0, square=True, linewidths=1)\n",
    "    plt.title('Feature Correlation Heatmap', fontsize=14, fontweight='bold')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Sales vs Quality Scatter Plots\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    if 'Global_Sales' in df.columns and 'Critic_Score' in df.columns:\n",
    "        axes[0].scatter(df['Critic_Score'], df['Global_Sales'], alpha=0.5, s=20)\n",
    "        axes[0].set_xlabel('Critic Score')\n",
    "        axes[0].set_ylabel('Global Sales (millions)')\n",
    "        axes[0].set_title('Global Sales vs Critic Score')\n",
    "        axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    if 'Global_Sales' in df.columns and 'User_Score' in df.columns:\n",
    "        axes[1].scatter(df['User_Score'], df['Global_Sales'], alpha=0.5, s=20, color='orange')\n",
    "        axes[1].set_xlabel('User Score')\n",
    "        axes[1].set_ylabel('Global Sales (millions)')\n",
    "        axes[1].set_title('Global Sales vs User Score')\n",
    "        axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Critic vs User Score Agreement\n",
    "    if 'Critic_Score' in df.columns and 'User_Score_Normalized' in df.columns:\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        plt.scatter(df['Critic_Score'], df['User_Score_Normalized'], alpha=0.4, s=30)\n",
    "        plt.plot([0, 100], [0, 100], 'r--', linewidth=2, label='Perfect Agreement')\n",
    "        plt.xlabel('Critic Score')\n",
    "        plt.ylabel('User Score (Normalized)')\n",
    "        plt.title('Critic vs User Score Agreement', fontsize=14, fontweight='bold')\n",
    "        plt.legend()\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Calculate correlation\n",
    "        corr = df[['Critic_Score', 'User_Score_Normalized']].corr().iloc[0, 1]\n",
    "        print(f\"\\nCorrelation between Critic and User Scores: {corr:.3f}\")\n",
    "\n",
    "# Run EDA\n",
    "try:\n",
    "    visualize_eda(df_features)\n",
    "except NameError:\n",
    "    print(\"Dataset with features not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee59e772",
   "metadata": {},
   "source": [
    "## Step 5: Feature Scaling and Dimensionality Reduction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e65bdb11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset with features not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def prepare_clustering_data(df):\n",
    "    \"\"\"\n",
    "    Select features for clustering and apply standardization\n",
    "    \"\"\"\n",
    "    \n",
    "    # Select features for clustering\n",
    "    # Focus on: Sales, Quality Scores, and their interactions\n",
    "    clustering_features = [\n",
    "        'Log_Global_Sales',        # Commercial success (log-transformed)\n",
    "        'Avg_Quality_Score',       # Overall quality (critic + user average)\n",
    "        'Critic_User_Gap',         # Agreement between critics and users\n",
    "        'Sales_Quality_Ratio',     # Success relative to quality\n",
    "    ]\n",
    "    \n",
    "    # Check which features are available\n",
    "    available_features = [f for f in clustering_features if f in df.columns]\n",
    "    \n",
    "    print(f\"Features selected for clustering:\")\n",
    "    for feat in available_features:\n",
    "        print(f\"  - {feat}\")\n",
    "    \n",
    "    # Extract feature matrix\n",
    "    X = df[available_features].values\n",
    "    \n",
    "    # Standardize features (mean=0, std=1)\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    \n",
    "    print(f\"\\nFeature matrix shape: {X_scaled.shape}\")\n",
    "    print(f\"Features are now standardized (mean ‚âà 0, std ‚âà 1)\")\n",
    "    \n",
    "    return X_scaled, available_features, scaler\n",
    "\n",
    "# Apply PCA for visualization\n",
    "def apply_pca(X_scaled, n_components=3):\n",
    "    \"\"\"\n",
    "    Apply PCA for dimensionality reduction and visualization\n",
    "    \"\"\"\n",
    "    pca = PCA(n_components=n_components)\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    \n",
    "    print(f\"\\nPCA Analysis:\")\n",
    "    print(f\"Number of components: {n_components}\")\n",
    "    print(f\"Explained variance ratio: {pca.explained_variance_ratio_}\")\n",
    "    print(f\"Cumulative explained variance: {np.cumsum(pca.explained_variance_ratio_)}\")\n",
    "    \n",
    "    # Visualize explained variance\n",
    "    plt.figure(figsize=(10, 5))\n",
    "    plt.bar(range(1, n_components + 1), pca.explained_variance_ratio_, alpha=0.7, color='steelblue')\n",
    "    plt.plot(range(1, n_components + 1), np.cumsum(pca.explained_variance_ratio_), \n",
    "             'ro-', linewidth=2, markersize=8, label='Cumulative')\n",
    "    plt.xlabel('Principal Component')\n",
    "    plt.ylabel('Explained Variance Ratio')\n",
    "    plt.title('PCA - Explained Variance by Component')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return X_pca, pca\n",
    "\n",
    "# Prepare data for clustering\n",
    "try:\n",
    "    X_scaled, feature_names, scaler = prepare_clustering_data(df_features)\n",
    "    X_pca, pca = apply_pca(X_scaled, n_components=3)\n",
    "    \n",
    "    print(f\"\\n‚úì Data preparation complete!\")\n",
    "    print(f\"  Ready for clustering with {X_scaled.shape[0]} games and {X_scaled.shape[1]} features\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Dataset with features not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c13e712",
   "metadata": {},
   "source": [
    "## Step 6: Determine Optimal Number of Clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8f6a4fef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def find_optimal_clusters(X, max_k=10):\n",
    "    \"\"\"\n",
    "    Use multiple methods to determine optimal number of clusters:\n",
    "    1. Elbow Method (Within-Cluster Sum of Squares)\n",
    "    2. Silhouette Score\n",
    "    3. Davies-Bouldin Index\n",
    "    \"\"\"\n",
    "    \n",
    "    # Test different numbers of clusters\n",
    "    k_range = range(2, max_k + 1)\n",
    "    \n",
    "    wcss = []  # Within-cluster sum of squares\n",
    "    silhouette_scores = []\n",
    "    davies_bouldin_scores = []\n",
    "    \n",
    "    print(\"Testing different numbers of clusters...\")\n",
    "    \n",
    "    for k in k_range:\n",
    "        # Fit K-Means\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "        labels = kmeans.fit_predict(X)\n",
    "        \n",
    "        # Calculate metrics\n",
    "        wcss.append(kmeans.inertia_)\n",
    "        silhouette_scores.append(silhouette_score(X, labels))\n",
    "        davies_bouldin_scores.append(davies_bouldin_score(X, labels))\n",
    "        \n",
    "        print(f\"k={k}: Silhouette={silhouette_scores[-1]:.3f}, Davies-Bouldin={davies_bouldin_scores[-1]:.3f}\")\n",
    "    \n",
    "    # Visualize all methods\n",
    "    fig, axes = plt.subplots(1, 3, figsize=(18, 5))\n",
    "    \n",
    "    # 1. Elbow Method\n",
    "    axes[0].plot(k_range, wcss, 'bo-', linewidth=2, markersize=8)\n",
    "    axes[0].set_xlabel('Number of Clusters (k)')\n",
    "    axes[0].set_ylabel('Within-Cluster Sum of Squares (WCSS)')\n",
    "    axes[0].set_title('Elbow Method')\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # 2. Silhouette Score (higher is better)\n",
    "    axes[1].plot(k_range, silhouette_scores, 'go-', linewidth=2, markersize=8)\n",
    "    axes[1].set_xlabel('Number of Clusters (k)')\n",
    "    axes[1].set_ylabel('Silhouette Score')\n",
    "    axes[1].set_title('Silhouette Analysis (Higher = Better)')\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    # 3. Davies-Bouldin Index (lower is better)\n",
    "    axes[2].plot(k_range, davies_bouldin_scores, 'ro-', linewidth=2, markersize=8)\n",
    "    axes[2].set_xlabel('Number of Clusters (k)')\n",
    "    axes[2].set_ylabel('Davies-Bouldin Index')\n",
    "    axes[2].set_title('Davies-Bouldin Index (Lower = Better)')\n",
    "    axes[2].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Recommend optimal k\n",
    "    optimal_k_silhouette = k_range[np.argmax(silhouette_scores)]\n",
    "    optimal_k_db = k_range[np.argmin(davies_bouldin_scores)]\n",
    "    \n",
    "    print(f\"\\nüìä Recommended number of clusters:\")\n",
    "    print(f\"  - Based on Silhouette Score: k = {optimal_k_silhouette}\")\n",
    "    print(f\"  - Based on Davies-Bouldin Index: k = {optimal_k_db}\")\n",
    "    print(f\"  - Suggested: k = {optimal_k_silhouette} (can be adjusted based on domain knowledge)\")\n",
    "    \n",
    "    return optimal_k_silhouette\n",
    "\n",
    "# Hierarchical Clustering Dendrogram\n",
    "def plot_dendrogram(X, method='ward'):\n",
    "    \"\"\"\n",
    "    Create dendrogram to visualize hierarchical clustering\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(14, 6))\n",
    "    \n",
    "    # Calculate linkage\n",
    "    linkage_matrix = linkage(X, method=method)\n",
    "    \n",
    "    # Plot dendrogram\n",
    "    dendrogram(linkage_matrix, truncate_mode='lastp', p=30)\n",
    "    plt.xlabel('Sample Index or (Cluster Size)')\n",
    "    plt.ylabel('Distance')\n",
    "    plt.title(f'Hierarchical Clustering Dendrogram ({method.capitalize()} Linkage)')\n",
    "    plt.axhline(y=np.median(linkage_matrix[:, 2]), color='r', linestyle='--', \n",
    "                label='Suggested Cut Height')\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Find optimal number of clusters\n",
    "try:\n",
    "    optimal_k = find_optimal_clusters(X_scaled, max_k=10)\n",
    "    print(f\"\\n\" + \"=\"*60)\n",
    "    print(f\"Proceeding with k = {optimal_k} clusters\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Also show dendrogram\n",
    "    print(\"\\nHierarchical Clustering Dendrogram:\")\n",
    "    plot_dendrogram(X_scaled)\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Scaled data not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ca1a0f1",
   "metadata": {},
   "source": [
    "## Step 7: Apply Clustering Algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e8311ac7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scaled data not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def apply_clustering_algorithms(X, k=4):\n",
    "    \"\"\"\n",
    "    Apply multiple clustering algorithms and compare results\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Applying clustering algorithms with k = {k} clusters...\\n\")\n",
    "    \n",
    "    # 1. K-Means Clustering\n",
    "    print(\"1. K-Means Clustering\")\n",
    "    kmeans = KMeans(n_clusters=k, random_state=42, n_init=10)\n",
    "    labels_kmeans = kmeans.fit_predict(X)\n",
    "    \n",
    "    silhouette_kmeans = silhouette_score(X, labels_kmeans)\n",
    "    davies_bouldin_kmeans = davies_bouldin_score(X, labels_kmeans)\n",
    "    calinski_kmeans = calinski_harabasz_score(X, labels_kmeans)\n",
    "    \n",
    "    print(f\"   - Silhouette Score: {silhouette_kmeans:.4f}\")\n",
    "    print(f\"   - Davies-Bouldin Index: {davies_bouldin_kmeans:.4f}\")\n",
    "    print(f\"   - Calinski-Harabasz Score: {calinski_kmeans:.4f}\")\n",
    "    print(f\"   - Cluster sizes: {np.bincount(labels_kmeans)}\")\n",
    "    \n",
    "    # 2. Hierarchical Clustering (Agglomerative)\n",
    "    print(\"\\n2. Hierarchical Clustering (Agglomerative)\")\n",
    "    hierarchical = AgglomerativeClustering(n_clusters=k, linkage='ward')\n",
    "    labels_hierarchical = hierarchical.fit_predict(X)\n",
    "    \n",
    "    silhouette_hierarchical = silhouette_score(X, labels_hierarchical)\n",
    "    davies_bouldin_hierarchical = davies_bouldin_score(X, labels_hierarchical)\n",
    "    calinski_hierarchical = calinski_harabasz_score(X, labels_hierarchical)\n",
    "    \n",
    "    print(f\"   - Silhouette Score: {silhouette_hierarchical:.4f}\")\n",
    "    print(f\"   - Davies-Bouldin Index: {davies_bouldin_hierarchical:.4f}\")\n",
    "    print(f\"   - Calinski-Harabasz Score: {calinski_hierarchical:.4f}\")\n",
    "    print(f\"   - Cluster sizes: {np.bincount(labels_hierarchical)}\")\n",
    "    \n",
    "    # 3. DBSCAN (Density-Based)\n",
    "    print(\"\\n3. DBSCAN Clustering\")\n",
    "    # Find optimal eps using k-distance graph\n",
    "    from sklearn.neighbors import NearestNeighbors\n",
    "    neighbors = NearestNeighbors(n_neighbors=k)\n",
    "    neighbors_fit = neighbors.fit(X)\n",
    "    distances, indices = neighbors_fit.kneighbors(X)\n",
    "    distances = np.sort(distances[:, -1], axis=0)\n",
    "    \n",
    "    # Use elbow point as eps (approximate)\n",
    "    eps = np.percentile(distances, 90)\n",
    "    \n",
    "    dbscan = DBSCAN(eps=eps, min_samples=k)\n",
    "    labels_dbscan = dbscan.fit_predict(X)\n",
    "    \n",
    "    n_clusters_dbscan = len(set(labels_dbscan)) - (1 if -1 in labels_dbscan else 0)\n",
    "    n_noise = list(labels_dbscan).count(-1)\n",
    "    \n",
    "    print(f\"   - Number of clusters: {n_clusters_dbscan}\")\n",
    "    print(f\"   - Number of noise points: {n_noise}\")\n",
    "    print(f\"   - eps parameter: {eps:.4f}\")\n",
    "    \n",
    "    if n_clusters_dbscan > 1:\n",
    "        # Only calculate if there are multiple clusters\n",
    "        valid_labels = labels_dbscan[labels_dbscan != -1]\n",
    "        valid_X = X[labels_dbscan != -1]\n",
    "        if len(set(valid_labels)) > 1:\n",
    "            silhouette_dbscan = silhouette_score(valid_X, valid_labels)\n",
    "            print(f\"   - Silhouette Score (excluding noise): {silhouette_dbscan:.4f}\")\n",
    "    \n",
    "    # Summary comparison\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ALGORITHM COMPARISON SUMMARY\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"{'Algorithm':<20} {'Silhouette':<15} {'Davies-Bouldin':<20} {'Calinski-Harabasz':<20}\")\n",
    "    print(\"-\"*75)\n",
    "    print(f\"{'K-Means':<20} {silhouette_kmeans:<15.4f} {davies_bouldin_kmeans:<20.4f} {calinski_kmeans:<20.4f}\")\n",
    "    print(f\"{'Hierarchical':<20} {silhouette_hierarchical:<15.4f} {davies_bouldin_hierarchical:<20.4f} {calinski_hierarchical:<20.4f}\")\n",
    "    print(\"=\"*75)\n",
    "    print(\"\\nNote: Higher Silhouette and Calinski-Harabasz, Lower Davies-Bouldin is better\")\n",
    "    \n",
    "    # Select best algorithm (K-Means typically works well for this type of data)\n",
    "    best_labels = labels_kmeans\n",
    "    best_model = kmeans\n",
    "    best_algorithm = \"K-Means\"\n",
    "    \n",
    "    print(f\"\\n‚úì Selected algorithm: {best_algorithm}\")\n",
    "    \n",
    "    return best_labels, best_model, best_algorithm, labels_kmeans, labels_hierarchical\n",
    "\n",
    "# Apply clustering\n",
    "try:\n",
    "    # Use the optimal k determined earlier (default to 4 if not found)\n",
    "    k_clusters = optimal_k if 'optimal_k' in locals() else 4\n",
    "    \n",
    "    cluster_labels, model, algorithm_name, kmeans_labels, hierarchical_labels = apply_clustering_algorithms(\n",
    "        X_scaled, k=k_clusters\n",
    "    )\n",
    "    \n",
    "    # Add cluster labels to the dataframe\n",
    "    df_features['Cluster'] = cluster_labels\n",
    "    df_features['Cluster_KMeans'] = kmeans_labels\n",
    "    df_features['Cluster_Hierarchical'] = hierarchical_labels\n",
    "    \n",
    "    print(f\"\\n‚úì Clustering complete! Cluster labels added to dataset.\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Scaled data not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6e0beef",
   "metadata": {},
   "source": [
    "## Step 8: Cluster Interpretation and Archetype Naming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "fdd4d3c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered dataset not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def interpret_clusters(df, cluster_col='Cluster'):\n",
    "    \"\"\"\n",
    "    Analyze cluster characteristics and assign meaningful archetype names\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"CLUSTER ANALYSIS & ARCHETYPE NAMING\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Features to analyze\n",
    "    analysis_features = ['Global_Sales', 'Critic_Score', 'User_Score', \n",
    "                        'Avg_Quality_Score', 'Critic_User_Gap', 'Sales_Quality_Ratio']\n",
    "    \n",
    "    # Check which features are available\n",
    "    available_features = [f for f in analysis_features if f in df.columns]\n",
    "    \n",
    "    # Calculate cluster statistics\n",
    "    cluster_stats = df.groupby(cluster_col)[available_features].agg(['mean', 'median', 'std'])\n",
    "    \n",
    "    print(f\"\\nCluster Statistics (Mean Values):\")\n",
    "    print(\"-\"*80)\n",
    "    display(df.groupby(cluster_col)[available_features].mean().round(3))\n",
    "    \n",
    "    # Assign archetype names based on characteristics\n",
    "    archetypes = {}\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        \n",
    "        avg_sales = cluster_data['Global_Sales'].mean() if 'Global_Sales' in cluster_data.columns else 0\n",
    "        avg_quality = cluster_data['Avg_Quality_Score'].mean() if 'Avg_Quality_Score' in cluster_data.columns else 0\n",
    "        critic_score = cluster_data['Critic_Score'].mean() if 'Critic_Score' in cluster_data.columns else 0\n",
    "        user_score = cluster_data['User_Score'].mean() if 'User_Score' in cluster_data.columns else 0\n",
    "        critic_user_gap = cluster_data['Critic_User_Gap'].mean() if 'Critic_User_Gap' in cluster_data.columns else 0\n",
    "        \n",
    "        # Determine archetype based on sales and quality\n",
    "        if avg_sales > df['Global_Sales'].quantile(0.75) and avg_quality > df['Avg_Quality_Score'].quantile(0.75):\n",
    "            archetype = \"üèÜ Critically Acclaimed Blockbusters\"\n",
    "            description = \"High sales + High quality - The gold standard\"\n",
    "        \n",
    "        elif avg_sales < df['Global_Sales'].quantile(0.25) and avg_quality > df['Avg_Quality_Score'].quantile(0.75):\n",
    "            archetype = \"üíé Underrated Gems\"\n",
    "            description = \"Low sales + High quality - Hidden masterpieces\"\n",
    "        \n",
    "        elif avg_sales > df['Global_Sales'].quantile(0.75) and avg_quality < df['Avg_Quality_Score'].quantile(0.25):\n",
    "            archetype = \"üí∞ Commercial Hits\"\n",
    "            description = \"High sales + Low quality - Popular but criticized\"\n",
    "        \n",
    "        elif avg_sales < df['Global_Sales'].quantile(0.25) and avg_quality < df['Avg_Quality_Score'].quantile(0.25):\n",
    "            archetype = \"üìâ Unsuccessful Titles\"\n",
    "            description = \"Low sales + Low quality - Market failures\"\n",
    "        \n",
    "        elif critic_user_gap > df['Critic_User_Gap'].quantile(0.75):\n",
    "            archetype = \"üé≠ Critic Favorites\"\n",
    "            description = \"Critics love them more than users\"\n",
    "        \n",
    "        elif critic_user_gap < df['Critic_User_Gap'].quantile(0.25):\n",
    "            archetype = \"üë• Fan Favorites\"\n",
    "            description = \"Users love them more than critics\"\n",
    "        \n",
    "        else:\n",
    "            archetype = \"‚öñÔ∏è Balanced Performers\"\n",
    "            description = \"Moderate sales and quality\"\n",
    "        \n",
    "        archetypes[cluster_id] = {\n",
    "            'name': archetype,\n",
    "            'description': description,\n",
    "            'size': len(cluster_data),\n",
    "            'avg_sales': avg_sales,\n",
    "            'avg_quality': avg_quality,\n",
    "            'critic_score': critic_score,\n",
    "            'user_score': user_score,\n",
    "            'critic_user_gap': critic_user_gap\n",
    "        }\n",
    "    \n",
    "    # Display archetype profiles\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"GAME ARCHETYPES DISCOVERED\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    for cluster_id, info in archetypes.items():\n",
    "        print(f\"\\n{'Cluster ' + str(cluster_id):=^80}\")\n",
    "        print(f\"{info['name']:^80}\")\n",
    "        print(f\"{info['description']:^80}\")\n",
    "        print(\"-\"*80)\n",
    "        print(f\"  Size: {info['size']} games\")\n",
    "        print(f\"  Avg Global Sales: {info['avg_sales']:.2f} million\")\n",
    "        print(f\"  Avg Quality Score: {info['avg_quality']:.2f}/100\")\n",
    "        print(f\"  Avg Critic Score: {info['critic_score']:.2f}/100\")\n",
    "        print(f\"  Avg User Score: {info['user_score']:.2f}/10\")\n",
    "        print(f\"  Critic-User Gap: {info['critic_user_gap']:.2f}\")\n",
    "        \n",
    "        # Show some example games\n",
    "        if 'Name' in df.columns:\n",
    "            print(f\"\\n  Example Games:\")\n",
    "            examples = cluster_data.nlargest(3, 'Global_Sales')['Name'].values if 'Global_Sales' in cluster_data.columns else cluster_data.head(3)['Name'].values\n",
    "            for i, game in enumerate(examples, 1):\n",
    "                print(f\"    {i}. {game}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    \n",
    "    return archetypes\n",
    "\n",
    "# Interpret clusters\n",
    "try:\n",
    "    archetypes = interpret_clusters(df_features)\n",
    "    print(\"\\n‚úì Cluster interpretation complete!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Clustered dataset not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3986f02",
   "metadata": {},
   "source": [
    "## Step 9: Cluster Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "fe4febef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Required data not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def visualize_clusters(df, X_pca, cluster_col='Cluster'):\n",
    "    \"\"\"\n",
    "    Create comprehensive visualizations of the clusters\n",
    "    \"\"\"\n",
    "    \n",
    "    # Color palette\n",
    "    colors = plt.cm.Set3(np.linspace(0, 1, len(df[cluster_col].unique())))\n",
    "    \n",
    "    # 1. 2D PCA Visualization\n",
    "    fig, axes = plt.subplots(1, 2, figsize=(18, 7))\n",
    "    \n",
    "    # PC1 vs PC2\n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        mask = df[cluster_col] == cluster_id\n",
    "        axes[0].scatter(X_pca[mask, 0], X_pca[mask, 1], \n",
    "                       label=f'Cluster {cluster_id}', alpha=0.6, s=50, \n",
    "                       color=colors[cluster_id])\n",
    "    \n",
    "    axes[0].set_xlabel('First Principal Component')\n",
    "    axes[0].set_ylabel('Second Principal Component')\n",
    "    axes[0].set_title('Clusters in PCA Space (PC1 vs PC2)')\n",
    "    axes[0].legend()\n",
    "    axes[0].grid(alpha=0.3)\n",
    "    \n",
    "    # PC2 vs PC3\n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        mask = df[cluster_col] == cluster_id\n",
    "        axes[1].scatter(X_pca[mask, 1], X_pca[mask, 2], \n",
    "                       label=f'Cluster {cluster_id}', alpha=0.6, s=50, \n",
    "                       color=colors[cluster_id])\n",
    "    \n",
    "    axes[1].set_xlabel('Second Principal Component')\n",
    "    axes[1].set_ylabel('Third Principal Component')\n",
    "    axes[1].set_title('Clusters in PCA Space (PC2 vs PC3)')\n",
    "    axes[1].legend()\n",
    "    axes[1].grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 2. 3D Visualization\n",
    "    from mpl_toolkits.mplot3d import Axes3D\n",
    "    \n",
    "    fig = plt.figure(figsize=(14, 10))\n",
    "    ax = fig.add_subplot(111, projection='3d')\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        mask = df[cluster_col] == cluster_id\n",
    "        ax.scatter(X_pca[mask, 0], X_pca[mask, 1], X_pca[mask, 2],\n",
    "                  label=f'Cluster {cluster_id}', alpha=0.6, s=50,\n",
    "                  color=colors[cluster_id])\n",
    "    \n",
    "    ax.set_xlabel('PC1')\n",
    "    ax.set_ylabel('PC2')\n",
    "    ax.set_zlabel('PC3')\n",
    "    ax.set_title('3D Cluster Visualization (PCA Space)', fontsize=14, fontweight='bold')\n",
    "    ax.legend()\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 3. Feature Space Visualization (Sales vs Quality)\n",
    "    fig, axes = plt.subplots(2, 2, figsize=(16, 14))\n",
    "    \n",
    "    # Sales vs Avg Quality Score\n",
    "    if 'Global_Sales' in df.columns and 'Avg_Quality_Score' in df.columns:\n",
    "        for cluster_id in sorted(df[cluster_col].unique()):\n",
    "            mask = df[cluster_col] == cluster_id\n",
    "            axes[0, 0].scatter(df[mask]['Avg_Quality_Score'], df[mask]['Global_Sales'],\n",
    "                             label=f'Cluster {cluster_id}', alpha=0.6, s=50,\n",
    "                             color=colors[cluster_id])\n",
    "        axes[0, 0].set_xlabel('Average Quality Score')\n",
    "        axes[0, 0].set_ylabel('Global Sales (millions)')\n",
    "        axes[0, 0].set_title('Sales vs Quality by Cluster')\n",
    "        axes[0, 0].legend()\n",
    "        axes[0, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Critic Score vs User Score\n",
    "    if 'Critic_Score' in df.columns and 'User_Score' in df.columns:\n",
    "        for cluster_id in sorted(df[cluster_col].unique()):\n",
    "            mask = df[cluster_col] == cluster_id\n",
    "            axes[0, 1].scatter(df[mask]['Critic_Score'], df[mask]['User_Score'],\n",
    "                             label=f'Cluster {cluster_id}', alpha=0.6, s=50,\n",
    "                             color=colors[cluster_id])\n",
    "        axes[0, 1].plot([0, 100], [0, 10], 'r--', alpha=0.5, label='Perfect Agreement')\n",
    "        axes[0, 1].set_xlabel('Critic Score')\n",
    "        axes[0, 1].set_ylabel('User Score')\n",
    "        axes[0, 1].set_title('Critic vs User Score by Cluster')\n",
    "        axes[0, 1].legend()\n",
    "        axes[0, 1].grid(alpha=0.3)\n",
    "    \n",
    "    # Log Sales vs Quality\n",
    "    if 'Log_Global_Sales' in df.columns and 'Avg_Quality_Score' in df.columns:\n",
    "        for cluster_id in sorted(df[cluster_col].unique()):\n",
    "            mask = df[cluster_col] == cluster_id\n",
    "            axes[1, 0].scatter(df[mask]['Avg_Quality_Score'], df[mask]['Log_Global_Sales'],\n",
    "                             label=f'Cluster {cluster_id}', alpha=0.6, s=50,\n",
    "                             color=colors[cluster_id])\n",
    "        axes[1, 0].set_xlabel('Average Quality Score')\n",
    "        axes[1, 0].set_ylabel('Log(Global Sales)')\n",
    "        axes[1, 0].set_title('Log Sales vs Quality by Cluster')\n",
    "        axes[1, 0].legend()\n",
    "        axes[1, 0].grid(alpha=0.3)\n",
    "    \n",
    "    # Cluster Size Distribution\n",
    "    cluster_sizes = df[cluster_col].value_counts().sort_index()\n",
    "    axes[1, 1].bar(cluster_sizes.index, cluster_sizes.values, color=colors, alpha=0.7)\n",
    "    axes[1, 1].set_xlabel('Cluster ID')\n",
    "    axes[1, 1].set_ylabel('Number of Games')\n",
    "    axes[1, 1].set_title('Cluster Size Distribution')\n",
    "    axes[1, 1].grid(axis='y', alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # 4. Box plots for each feature by cluster\n",
    "    numeric_features = ['Global_Sales', 'Critic_Score', 'User_Score', 'Critic_User_Gap']\n",
    "    available_features = [f for f in numeric_features if f in df.columns]\n",
    "    \n",
    "    if available_features:\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "        axes = axes.flatten()\n",
    "        \n",
    "        for idx, feature in enumerate(available_features):\n",
    "            if idx < 4:\n",
    "                df.boxplot(column=feature, by=cluster_col, ax=axes[idx])\n",
    "                axes[idx].set_xlabel('Cluster')\n",
    "                axes[idx].set_ylabel(feature)\n",
    "                axes[idx].set_title(f'{feature} Distribution by Cluster')\n",
    "                plt.sca(axes[idx])\n",
    "                plt.xticks(rotation=0)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "# Visualize clusters\n",
    "try:\n",
    "    visualize_clusters(df_features, X_pca)\n",
    "    print(\"‚úì Cluster visualization complete!\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Required data not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10f2ddca",
   "metadata": {},
   "source": [
    "## Step 10: Final Analysis & Conclusions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9833a1c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Clustered data not available. Please run previous cells first.\n"
     ]
    }
   ],
   "source": [
    "def generate_final_report(df, cluster_col='Cluster'):\n",
    "    \"\"\"\n",
    "    Generate comprehensive final report and insights\n",
    "    \"\"\"\n",
    "    \n",
    "    print(\"=\"*80)\n",
    "    print(\"FINAL ANALYSIS & KEY INSIGHTS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # 1. Overall Statistics\n",
    "    print(\"\\n1. DATASET OVERVIEW\")\n",
    "    print(\"-\"*80)\n",
    "    print(f\"Total games analyzed: {len(df)}\")\n",
    "    print(f\"Number of clusters: {df[cluster_col].nunique()}\")\n",
    "    print(f\"Date of analysis: {pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    \n",
    "    # 2. Key Findings\n",
    "    print(\"\\n2. KEY FINDINGS\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    if 'Critic_Score' in df.columns and 'User_Score_Normalized' in df.columns:\n",
    "        correlation = df[['Critic_Score', 'User_Score_Normalized']].corr().iloc[0, 1]\n",
    "        print(f\"‚úì Critic-User Score Correlation: {correlation:.3f}\")\n",
    "        if correlation > 0.7:\n",
    "            print(\"  ‚Üí Strong positive correlation: Critics and users generally agree\")\n",
    "        elif correlation > 0.4:\n",
    "            print(\"  ‚Üí Moderate correlation: Some agreement between critics and users\")\n",
    "        else:\n",
    "            print(\"  ‚Üí Weak correlation: Significant discrepancy between critics and users\")\n",
    "    \n",
    "    if 'Global_Sales' in df.columns and 'Avg_Quality_Score' in df.columns:\n",
    "        sales_quality_corr = df[['Global_Sales', 'Avg_Quality_Score']].corr().iloc[0, 1]\n",
    "        print(f\"\\n‚úì Sales-Quality Correlation: {sales_quality_corr:.3f}\")\n",
    "        if sales_quality_corr > 0.5:\n",
    "            print(\"  ‚Üí Higher quality games tend to sell better\")\n",
    "        elif sales_quality_corr > 0:\n",
    "            print(\"  ‚Üí Weak positive relationship between quality and sales\")\n",
    "        else:\n",
    "            print(\"  ‚Üí Quality doesn't guarantee commercial success\")\n",
    "    \n",
    "    # 3. Cluster Insights\n",
    "    print(\"\\n3. DISCOVERED ARCHETYPES\")\n",
    "    print(\"-\"*80)\n",
    "    \n",
    "    for cluster_id in sorted(df[cluster_col].unique()):\n",
    "        cluster_data = df[df[cluster_col] == cluster_id]\n",
    "        size_pct = (len(cluster_data) / len(df)) * 100\n",
    "        \n",
    "        print(f\"\\nCluster {cluster_id} ({len(cluster_data)} games, {size_pct:.1f}% of dataset)\")\n",
    "        \n",
    "        if 'Global_Sales' in cluster_data.columns:\n",
    "            print(f\"  Avg Sales: {cluster_data['Global_Sales'].mean():.2f}M\")\n",
    "        if 'Avg_Quality_Score' in cluster_data.columns:\n",
    "            print(f\"  Avg Quality: {cluster_data['Avg_Quality_Score'].mean():.2f}/100\")\n",
    "        if 'Critic_User_Gap' in cluster_data.columns:\n",
    "            print(f\"  Critic-User Gap: {cluster_data['Critic_User_Gap'].mean():.2f}\")\n",
    "    \n",
    "    # 4. Business Insights\n",
    "    print(\"\\n4. BUSINESS INSIGHTS\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"‚úì Use archetypes to:\")\n",
    "    print(\"  ‚Ä¢ Identify market gaps and opportunities\")\n",
    "    print(\"  ‚Ä¢ Understand what makes games commercially successful\")\n",
    "    print(\"  ‚Ä¢ Discover undervalued games (Underrated Gems)\")\n",
    "    print(\"  ‚Ä¢ Learn from successful patterns (Critically Acclaimed Blockbusters)\")\n",
    "    print(\"  ‚Ä¢ Avoid pitfalls (analyze unsuccessful clusters)\")\n",
    "    \n",
    "    # 5. Recommendations\n",
    "    print(\"\\n5. RECOMMENDATIONS\")\n",
    "    print(\"-\"*80)\n",
    "    print(\"For Game Developers:\")\n",
    "    print(\"  ‚Ä¢ Study games in 'Critically Acclaimed Blockbusters' cluster\")\n",
    "    print(\"  ‚Ä¢ Address critic-user discrepancies in your genre\")\n",
    "    print(\"  ‚Ä¢ Quality matters, but marketing is also crucial\")\n",
    "    \n",
    "    print(\"\\nFor Gamers:\")\n",
    "    print(\"  ‚Ä¢ Explore 'Underrated Gems' for high-quality hidden titles\")\n",
    "    print(\"  ‚Ä¢ Consider both critic and user scores for balanced view\")\n",
    "    \n",
    "    print(\"\\nFor Investors:\")\n",
    "    print(\"  ‚Ä¢ Focus on patterns that combine quality and sales potential\")\n",
    "    print(\"  ‚Ä¢ Monitor critic-user alignment as indicator of market reception\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ANALYSIS COMPLETE\")\n",
    "    print(\"=\"*80)\n",
    "\n",
    "# Generate final report\n",
    "try:\n",
    "    generate_final_report(df_features)\n",
    "    \n",
    "    print(\"\\n‚úÖ PROJECT COMPLETE!\")\n",
    "    print(\"\\nNext Steps:\")\n",
    "    print(\"  1. Download the dataset from Kaggle\")\n",
    "    print(\"  2. Run all cells in sequence to see full analysis\")\n",
    "    print(\"  3. Adjust number of clusters (k) based on your needs\")\n",
    "    print(\"  4. Export results for further use\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Clustered data not available. Please run previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1869d7f2",
   "metadata": {},
   "source": [
    "## Optional: Export Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "241693a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data not available for export. Please run all previous cells first.\n"
     ]
    }
   ],
   "source": [
    "# Export clustered data to CSV\n",
    "try:\n",
    "    output_file = 'video_games_clustered_results.csv'\n",
    "    \n",
    "    # Select important columns for export\n",
    "    export_cols = ['Name', 'Platform', 'Year_of_Release', 'Genre', \n",
    "                   'Global_Sales', 'Critic_Score', 'User_Score',\n",
    "                   'Avg_Quality_Score', 'Critic_User_Gap', 'Cluster']\n",
    "    \n",
    "    # Filter to existing columns\n",
    "    export_cols = [col for col in export_cols if col in df_features.columns]\n",
    "    \n",
    "    df_export = df_features[export_cols].copy()\n",
    "    df_export.to_csv(output_file, index=False)\n",
    "    \n",
    "    print(f\"‚úì Results exported to '{output_file}'\")\n",
    "    print(f\"  Columns exported: {', '.join(export_cols)}\")\n",
    "    print(f\"  Total rows: {len(df_export)}\")\n",
    "    \n",
    "except NameError:\n",
    "    print(\"Data not available for export. Please run all previous cells first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "383221a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üìã Instructions to Run This Project\n",
    "\n",
    "### 1. Download the Dataset\n",
    "- Go to: https://www.kaggle.com/datasets/rush4ratio/video-game-sales-with-ratings/data\n",
    "- Download: `Video_Games_Sales_as_at_22_Dec_2016.csv`\n",
    "- Place the file in the same directory as this notebook\n",
    "\n",
    "### 2. Run All Cells\n",
    "- Click \"Run All\" or execute cells sequentially from top to bottom\n",
    "- The analysis will automatically:\n",
    "  - Load and clean the data\n",
    "  - Engineer meaningful features\n",
    "  - Determine optimal number of clusters\n",
    "  - Apply multiple clustering algorithms\n",
    "  - Generate visualizations and insights\n",
    "\n",
    "### 3. Key Features of This Analysis\n",
    "\n",
    "**Feature Engineering:**\n",
    "- Log-transformed sales (handles skewness)\n",
    "- Average quality score (critic + user consensus)\n",
    "- Critic-user gap (agreement measure)\n",
    "- Sales-quality ratio (success relative to quality)\n",
    "\n",
    "**Clustering Methods:**\n",
    "- K-Means (partition-based)\n",
    "- Hierarchical Clustering (dendrogram-based)\n",
    "- DBSCAN (density-based, detects outliers)\n",
    "\n",
    "**Evaluation Metrics:**\n",
    "- Silhouette Score (higher = better separation)\n",
    "- Davies-Bouldin Index (lower = better clustering)\n",
    "- Calinski-Harabasz Score (higher = better defined clusters)\n",
    "\n",
    "### 4. Expected Archetypes\n",
    "\n",
    "The model will discover archetypes such as:\n",
    "- üèÜ **Critically Acclaimed Blockbusters**: High sales + High quality\n",
    "- üíé **Underrated Gems**: Low sales + High quality\n",
    "- üí∞ **Commercial Hits**: High sales + Lower quality\n",
    "- üìâ **Unsuccessful Titles**: Low sales + Low quality\n",
    "- üé≠ **Critic Favorites**: Critics love more than users\n",
    "- üë• **Fan Favorites**: Users love more than critics\n",
    "\n",
    "### 5. Customization Options\n",
    "\n",
    "You can adjust:\n",
    "- `max_k` in Step 6 to test more cluster numbers\n",
    "- `k_clusters` in Step 7 to change the final number of clusters\n",
    "- Features in Step 5 to focus on different aspects\n",
    "\n",
    "### 6. Research Questions Answered\n",
    "\n",
    "‚úì Do high critic scores align with commercial success?\n",
    "‚úì How do user-critic discrepancies define game clusters?\n",
    "‚úì What patterns exist between quality and sales?\n",
    "‚úì Can we discover \"hidden gems\" through clustering?\n",
    "\n",
    "---\n",
    "\n",
    "## üìö Technical Details\n",
    "\n",
    "**Machine Learning Approach:**\n",
    "- Unsupervised Learning (no labels needed)\n",
    "- Feature standardization (zero mean, unit variance)\n",
    "- Dimensionality reduction with PCA\n",
    "- Multiple algorithm comparison\n",
    "\n",
    "**Libraries Used:**\n",
    "- pandas, numpy (data manipulation)\n",
    "- scikit-learn (ML algorithms)\n",
    "- matplotlib, seaborn (visualization)\n",
    "- scipy (hierarchical clustering)\n",
    "\n",
    "**Dataset Attributes:**\n",
    "- Commercial: Global_Sales, Regional Sales\n",
    "- Critical: Critic_Score, User_Score\n",
    "- Metadata: Name, Platform, Genre, Year\n",
    "\n",
    "---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
