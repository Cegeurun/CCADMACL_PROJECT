
\documentclass[conference]{IEEEtran}
\usepackage{url}
\usepackage{hyperref}
\usepackage{amsmath}
\usepackage{amssymb}
% If IEEEtran.cls has not been installed into the LaTeX system files,
% manually specify the path to it like:
% \documentclass[journal]{../sty/IEEEtran}





% Some very useful LaTeX packages include:
% (uncomment the ones you want to load)


% *** MISC UTILITY PACKAGES ***
\usepackage{graphicx}
%
%\usepackage{ifpdf}
% Heiko Oberdiek's ifpdf.sty is very useful if you need conditional
% compilation based on whether the output is pdf or dvi.
% usage:
% \ifpdf
%   % pdf code
% \else
%   % dvi code
% \fi
% The latest version of ifpdf.sty can be obtained from:
% http://www.ctan.org/pkg/ifpdf
% Also, note that IEEEtran.cls V1.7 and later provides a builtin
% \ifCLASSINFOpdf conditional that works the same way.
% When switching from latex to pdflatex and vice-versa, the compiler may
% have to be run twice to clear warning/error messages.






% *** CITATION PACKAGES ***
%
%\usepackage{cite}
% cite.sty was written by Donald Arseneau
% V1.6 and later of IEEEtran pre-defines the format of the cite.sty package
% \cite{} output to follow that of the IEEE. Loading the cite package will
% result in citation numbers being automatically sorted and properly
% "compressed/ranged". e.g., [1], [9], [2], [7], [5], [6] without using
% cite.sty will become [1], [2], [5]--[7], [9] using cite.sty. cite.sty's
% \cite will automatically add leading space, if needed. Use cite.sty's
% noadjust option (cite.sty V3.8 and later) if you want to turn this off
% such as if a citation ever needs to be enclosed in parenthesis.
% cite.sty is already installed on most LaTeX systems. Be sure and use
% version 5.0 (2009-03-20) and later if using hyperref.sty.
% The latest version can be obtained at:
% http://www.ctan.org/pkg/cite
% The documentation is contained in the cite.sty file itself.






% *** GRAPHICS RELATED PACKAGES ***
%
\ifCLASSINFOpdf
  % \usepackage[pdftex]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../pdf/}{../jpeg/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.pdf,.jpeg,.png}
\else
  % or other class option (dvipsone, dvipdf, if not using dvips). graphicx
  % will default to the driver specified in the system graphics.cfg if no
  % driver is specified.
  % \usepackage[dvips]{graphicx}
  % declare the path(s) where your graphic files are
  % \graphicspath{{../eps/}}
  % and their extensions so you won't have to specify these with
  % every instance of \includegraphics
  % \DeclareGraphicsExtensions{.eps}
\fi
% graphicx was written by David Carlisle and Sebastian Rahtz. It is
% required if you want graphics, photos, etc. graphicx.sty is already
% installed on most LaTeX systems. The latest version and documentation
% can be obtained at: 
% http://www.ctan.org/pkg/graphicx
% Another good source of documentation is "Using Imported Graphics in
% LaTeX2e" by Keith Reckdahl which can be found at:
% http://www.ctan.org/pkg/epslatex
%
% latex, and pdflatex in dvi mode, support graphics in encapsulated
% postscript (.eps) format. pdflatex in pdf mode supports graphics
% in .pdf, .jpeg, .png and .mps (metapost) formats. Users should ensure
% that all non-photo figures use a vector format (.eps, .pdf, .mps) and
% not a bitmapped formats (.jpeg, .png). The IEEE frowns on bitmapped formats
% which can result in "jaggedy"/blurry rendering of lines and letters as
% well as large increases in file sizes.
%
% You can find documentation about the pdfTeX application at:
% http://www.tug.org/applications/pdftex





% *** MATH PACKAGES ***
%
%\usepackage{amsmath}
% A popular package from the American Mathematical Society that provides
% many useful and powerful commands for dealing with mathematics.
%
% Note that the amsmath package sets \interdisplaylinepenalty to 10000
% thus preventing page breaks from occurring within multiline equations. Use:
%\interdisplaylinepenalty=2500
% after loading amsmath to restore such page breaks as IEEEtran.cls normally
% does. amsmath.sty is already installed on most LaTeX systems. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/amsmath





% *** SPECIALIZED LIST PACKAGES ***
%
%\usepackage{algorithmic}
% algorithmic.sty was written by Peter Williams and Rogerio Brito.
% This package provides an algorithmic environment fo describing algorithms.
% You can use the algorithmic environment in-text or within a figure
% environment to provide for a floating algorithm. Do NOT use the algorithm
% floating environment provided by algorithm.sty (by the same authors) or
% algorithm2e.sty (by Christophe Fiorio) as the IEEE does not use dedicated
% algorithm float types and packages that provide these will not provide
% correct IEEE style captions. The latest version and documentation of
% algorithmic.sty can be obtained at:
% http://www.ctan.org/pkg/algorithms
% Also of interest may be the (relatively newer and more customizable)
% algorithmicx.sty package by Szasz Janos:
% http://www.ctan.org/pkg/algorithmicx




% *** ALIGNMENT PACKAGES ***
%
%\usepackage{array}
% Frank Mittelbach's and David Carlisle's array.sty patches and improves
% the standard LaTeX2e array and tabular environments to provide better
% appearance and additional user controls. As the default LaTeX2e table
% generation code is lacking to the point of almost being broken with
% respect to the quality of the end results, all users are strongly
% advised to use an enhanced (at the very least that provided by array.sty)
% set of table tools. array.sty is already installed on most systems. The
% latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/array


% IEEEtran contains the IEEEeqnarray family of commands that can be used to
% generate multiline equations as well as matrices, tables, etc., of high
% quality.




% *** SUBFIGURE PACKAGES ***
%\ifCLASSOPTIONcompsoc
%  \usepackage[caption=false,font=normalsize,labelfont=sf,textfont=sf]{subfig}
%\else
%  \usepackage[caption=false,font=footnotesize]{subfig}
%\fi
% subfig.sty, written by Steven Douglas Cochran, is the modern replacement
% for subfigure.sty, the latter of which is no longer maintained and is
% incompatible with some LaTeX packages including fixltx2e. However,
% subfig.sty requires and automatically loads Axel Sommerfeldt's caption.sty
% which will override IEEEtran.cls' handling of captions and this will result
% in non-IEEE style figure/table captions. To prevent this problem, be sure
% and invoke subfig.sty's "caption=false" package option (available since
% subfig.sty version 1.3, 2005/06/28) as this is will preserve IEEEtran.cls
% handling of captions.
% Note that the Computer Society format requires a larger sans serif font
% than the serif footnote size font used in traditional IEEE formatting
% and thus the need to invoke different subfig.sty package options depending
% on whether compsoc mode has been enabled.
%
% The latest version and documentation of subfig.sty can be obtained at:
% http://www.ctan.org/pkg/subfig




% *** FLOAT PACKAGES ***
%
%\usepackage{fixltx2e}
% fixltx2e, the successor to the earlier fix2col.sty, was written by
% Frank Mittelbach and David Carlisle. This package corrects a few problems
% in the LaTeX2e kernel, the most notable of which is that in current
% LaTeX2e releases, the ordering of single and double column floats is not
% guaranteed to be preserved. Thus, an unpatched LaTeX2e can allow a
% single column figure to be placed prior to an earlier double column
% figure.
% Be aware that LaTeX2e kernels dated 2015 and later have fixltx2e.sty's
% corrections already built into the system in which case a warning will
% be issued if an attempt is made to load fixltx2e.sty as it is no longer
% needed.
% The latest version and documentation can be found at:
% http://www.ctan.org/pkg/fixltx2e


%\usepackage{stfloats}
% stfloats.sty was written by Sigitas Tolusis. This package gives LaTeX2e
% the ability to do double column floats at the bottom of the page as well
% as the top. (e.g., "\begin{figure*}[!b]" is not normally possible in
% LaTeX2e). It also provides a command:
%\fnbelowfloat
% to enable the placement of footnotes below bottom floats (the standard
% LaTeX2e kernel puts them above bottom floats). This is an invasive package
% which rewrites many portions of the LaTeX2e float routines. It may not work
% with other packages that modify the LaTeX2e float routines. The latest
% version and documentation can be obtained at:
% http://www.ctan.org/pkg/stfloats
% Do not use the stfloats baselinefloat ability as the IEEE does not allow
% \baselineskip to stretch. Authors submitting work to the IEEE should note
% that the IEEE rarely uses double column equations and that authors should try
% to avoid such use. Do not be tempted to use the cuted.sty or midfloat.sty
% packages (also by Sigitas Tolusis) as the IEEE does not format its papers in
% such ways.
% Do not attempt to use stfloats with fixltx2e as they are incompatible.
% Instead, use Morten Hogholm'a dblfloatfix which combines the features
% of both fixltx2e and stfloats:
%
% \usepackage{dblfloatfix}
% The latest version can be found at:
% http://www.ctan.org/pkg/dblfloatfix




%\ifCLASSOPTIONcaptionsoff
%  \usepackage[nomarkers]{endfloat}
% \let\MYoriglatexcaption\caption
% \renewcommand{\caption}[2][\relax]{\MYoriglatexcaption[#2]{#2}}
%\fi
% endfloat.sty was written by James Darrell McCauley, Jeff Goldberg and 
% Axel Sommerfeldt. This package may be useful when used in conjunction with 
% IEEEtran.cls'  captionsoff option. Some IEEE journals/societies require that
% submissions have lists of figures/tables at the end of the paper and that
% figures/tables without any captions are placed on a page by themselves at
% the end of the document. If needed, the draftcls IEEEtran class option or
% \CLASSINPUTbaselinestretch interface can be used to increase the line
% spacing as well. Be sure and use the nomarkers option of endfloat to
% prevent endfloat from "marking" where the figures would have been placed
% in the text. The two hack lines of code above are a slight modification of
% that suggested by in the endfloat docs (section 8.4.1) to ensure that
% the full captions always appear in the list of figures/tables - even if
% the user used the short optional argument of \caption[]{}.
% IEEE papers do not typically make use of \caption[]'s optional argument,
% so this should not be an issue. A similar trick can be used to disable
% captions of packages such as subfig.sty that lack options to turn off
% the subcaptions:
% For subfig.sty:
% \let\MYorigsubfloat\subfloat
% \renewcommand{\subfloat}[2][\relax]{\MYorigsubfloat[]{#2}}
% However, the above trick will not work if both optional arguments of
% the \subfloat command are used. Furthermore, there needs to be a
% description of each subfigure *somewhere* and endfloat does not add
% subfigure captions to its list of figures. Thus, the best approach is to
% avoid the use of subfigure captions (many IEEE journals avoid them anyway)
% and instead reference/explain all the subfigures within the main caption.
% The latest version of endfloat.sty and its documentation can obtained at:
% http://www.ctan.org/pkg/endfloat
%
% The IEEEtran \ifCLASSOPTIONcaptionsoff conditional can also be used
% later in the document, say, to conditionally put the References on a 
% page by themselves.




% *** PDF, URL AND HYPERLINK PACKAGES ***
%
%\usepackage{url}
% url.sty was written by Donald Arseneau. It provides better support for
% handling and breaking URLs. url.sty is already installed on most LaTeX
% systems. The latest version and documentation can be obtained at:
% http://www.ctan.org/pkg/url
% Basically, \url{my_url_here}.




% *** Do not adjust lengths that control margins, column widths, etc. ***
% *** Do not use packages that alter fonts (such as pslatex).         ***
% There should be no need to do such things with IEEEtran.cls V1.6 and later.
% (Unless specifically asked to do so by the journal or conference you plan
% to submit to, of course. )


% correct bad hyphenation here
\hyphenation{op-tical net-works semi-conduc-tor}


\begin{document}
%
% paper title
% Titles are generally capitalized except for words such as a, an, and, as,
% at, but, by, for, in, nor, of, on, or, the, to and up, which are usually
% not capitalized unless they are the first or last word of the title.
% Linebreaks \\ can be used within to get better formatting as desired.
% Do not put math or special symbols in the title.

%=============================================
\title{Unsupervised Segmentation of Video Game Metadata for Quality Estimation  }


\author{\IEEEauthorblockN{Custer Jeremiah Valencerina} 
\IEEEauthorblockA{\textit{College of Computing and Information Technologies} \\
\textit{National University}\\
Manila, Philippines \\
valencerinacp@students.national-u.edu.ph}
\and
\IEEEauthorblockN{Gabriel Angelo Viñas} 
\IEEEauthorblockA{\textit{College of Computing and Information Technologies} \\
\textit{National University}\\
Manila, Philippines \\
vinasgm@national-u.edu.ph}
}

\maketitle

% As a general rule, do not put math, special symbols or citations
% in the abstract or keywords.
%=================================================
\begin{abstract}
\end{abstract}


\begin{IEEEkeywords}
Unsupervised learning, clustering algorithms, K-Means, Gaussian Mixture Models, hierarchical clustering, DBSCAN, dimensionality reduction, PCA, t-SNE, video game analytics, market segmentation
\end{IEEEkeywords}

\IEEEpeerreviewmaketitle
%=================================================
\section{Introduction}
%=================================================

The video game industry generates over \$180 billion in annual revenue, accompanied by the release  of thousands of new titles across diverse platforms. This expansive scale renders game quality assessment increasingly intricate and multifaceted \cite{1essentialfacts}. Conventional evaluation methods hinge on aggregated critic and user review scores, yet these measures often overlook the subtle interplay between critical reception and commercial performance \cite{2Cho}.

A key question arises: is there a consistent link between critical acclaim and commercial success in video games, or do specific market areas exist where quality and sales do not align? Prior studies in entertainment analytics indicate that the connection between reviews and sales is intricate and differs greatly depending on the situation \cite{3chiu2022critical}. Nevertheless, most current methods utilize supervised techniques that need pre-assigned labels or basic score compilations, which do not clearly categorize games into distinct archetypes.

This study addresses the challenge of automatically segmenting video games into quality archetypes by analyzing latent patterns between commercial performance and critical reception using unsupervised learning. We have rigorously implemented and compared four cluster algorithms, K-Means, Hierarchical agglomerative clustering, DBSCAN, and using data sets of 16,719 video games between 1980 and 2016 \cite{26Kirubi_2016}. Our approach discovers hidden patterns without the need for predefined categories, which is particularly valuable in emerging markets or in new game genres where labeled data sets are not available or prohibitively expensive to create.

This work has three contributions: (1) we propose the first comprehensive comparative assessment of a non-pervious clustering algorithm applied to game-level quality segmentation using commercial and critical metadata; (2) we design interpretation features that capture the dynamic relationship between critical ratings, user ratings, and commercial performance; and (3) we show that meaningful market archetypes come from metadata analysis alone, providing developers, publishers, and consumers with actionable insights to understand market positioning beyond simple ratings.

%=================================================
 \section{LITERATURE REVIEW}
 %================================================
This section establishes the theoretical foundation for unsupervised game quality segmentation, reviews key clustering methodologies, and positions our work within the broader context of video game analytics research.


\subsection{Overview of Key Concepts and Background Information}
 
 Unsupervised learning includes machine learning techniques that reveal hidden patterns in unlabeled data without requiring a predefined category \cite{7bengio2013representation}. Unlike supervised learning, which depends on costly labeled data sets, non-supervised approaches autonomously reveal the intrinsic structure and relationship within the data, an important advantage when personal labeling is subjective, costly, or impossible \cite{6hinton2006reducing}.

Clustering, an essential paradigm within unsupervised learning, systematically organizes data points based on their similarity while ensuring distinct separation among heterogeneous groups. Our analysis emphasizes four unique clustering methodologies, each possessing complementary advantages pertinent to the segmentation of game metadata: 

\subsubsection{K-Means Clustering}

 This partitioning technique operates by iteratively minimizing the within-cluster sum of squares (WCSS), achieved through the assignment of data points to the closest centroid followed by the recalibration of centroids \cite{8macqueen1967multivariate}. Despite its constraints regarding non-spherical clusters, its computational efficiency characterized by O(n·k·d·i) and inherent simplicity have established it as a cornerstone method. The k-means++ initialization technique markedly enhances convergence and solution quality by meticulously seeding the initial centroids \cite{9arthur2007k}.


 \subsubsection{Hierarchical Clustering}

This method constructs tree structures (dendrograms) through the iterative amalgamation of similar clusters, employing linkage criteria such as Ward's method (which minimizes variance), Complete linkage (which focuses on maximal distance), or Average linkage (which considers mean distance) \cite{10jain1988algorithms}. In contrast to K-Means, hierarchical clustering techniques do not necessitate the pre-specification of the number of clusters, thereby illuminating nested cluster relationships and offering profound insights into the structural complexities of the data across multiple levels of granularity.

\subsubsection{DBSCAN}

This algorithm delineates clusters through the examination of local density as opposed to distance metrics, effectively aggregating densely populated areas while categorizing solitary data points as noise or outliers \cite{12ester1996density}. DBSCAN exhibits the capacity to manage clusters of arbitrary geometrical configurations and autonomously ascertains the number of clusters; however, it faces challenges in scenarios characterized by heterogeneous densities and necessitates meticulous calibration of parameters, specifically the epsilon radius and the minimum number of points.

\subsection{Dimensionality Reduction and Evaluation}

Three complementary internal validation metrics are used to evaluate the quality of clustering: (1) the Silhouette Score \cite{18davies2009cluster}, which computes the average similarity between each cluster and its most similar cluster (with lower values indicating better separation); (2) the Davies-Bouldin Index \cite{19calinski1974dendrite}, which calculates the average similarity between each cluster and its most similar cluster (with values near 1 indicating well-separated, dense clusters); (3) Higher values denote better-defined clusters. The Calinski-Harabasz Score [13] calculates the ratio of between-cluster to within-cluster variance.

PCA, or principal component analysis, \cite{16abdi2010principal} carries out linear decrease of dimensionality through data projection onto orthogonal elements that capture the most variation. We use PCA. For both noise reduction and high-dimensional visualization, enhancing clustering effectiveness by concentrating on the most combinations of informative features.


% C. REVIEW OF OTHER RELEVANT RESEARCH PAPERS
\subsection{Review of Other Relevant Research Papers}
\subsubsection{Video Game Analytics Research}

Unsupervised learning has found considerable use in the study of video games, especially for uncovering hidden patterns in player behavior. Drachen et al. \cite{21drachen2012guns} utilized K-Means clustering on extensive behavioral telemetry data from commercial games, successfully identifying unique player profiles according to gameplay statistics without any preconceived notions of player categories. Sifa, Bauckhage, and Drachen \cite{22sifa2015user} built on this research by evaluating various clustering methods for user modeling, showing that different algorithms provide valuable and complementary insights into player segmentation.

Using large datasets from multiplayer online games, Bauckhage et al. \cite{23bauckhage2014how} investigated clustering approaches in further detail, showing how unsupervised learning can reveal behavioral patterns and involvement frameworks in intricate game settings. Their research demonstrates the usefulness and scalability of cluster-applying algorithms to huge entertainment datasets, a problem that markets level game metadata analysis also faces. In addition to video games, Gomez-Uribe and Hunt \cite{24gomez2016netflix} explained how unsupervised segmentation is more broadly applicable in digital content domains by using unsupervised modeling and latent factor techniques to shape entertainment catalogs in large-scale personalization systems.

Although there is a lot of study on player behavior cluster- utilizing game telemetry and doing little academic work directly use unsupervised clustering in commercial game-level important performance indicators. The majority of current research focuses on models for supervised sales prediction or behavioral analysis. Prior research on entertainment analytics has revealed intricate, non-linear connections between communication and critical reception commercial success \cite{4exploringvalueofproductreview}, taking into account the source, timeliness, and volume of reviews credibility all have an impact on how customers behave. We record this dynamic using our Score Discrepancy function, which quantifies The normalized disparity between user and critic ratings to determine which games have different audiences and professional reception.

This study shifts the analytical focus from players to games themselves, performing a comparative evaluation of multiple clustering algorithms (K-Means, Hierarchical Clustering, DBSCAN, and Gaussian Mixture Models) to segment video games based on sales and review metadata. By leveraging game-level metadata rather than individual player behavior, we contribute a novel market-oriented application of unsupervised learning within video game analytics. Our multi-algorithm comparison addresses the fundamental insight that no single clustering algorithm dominates across all datasets \cite{25xu2005survey},different algorithms excel with different data characteristics, making rigorous comparative evaluation essential for robust segmentation.

Unlike commercial systems that depend on untransparent collaboration. Our system can be used for basic score aggregation or collaborative filtering, it offers clear, understandable quality archetypes based on the discussed connection between commercial reception and critical performance. This automatic method of segmentation provides insights from data to help with strategic gaming decision-making systems for development, marketing, and recommendation.


\subsection{Prior Attempts and Our Approach}

Numerous researchers have tried using unsupervised studying video game statistics, but mostly concentrating on player behavior as opposed to market segmentation at the game level. K-Means clustering was used by Drachen et al. \cite{21drachen2012guns} to analyze player data from Tomb Raider: Underworld's behavioral telemetry, recognizing different play styles by looking at patterns of advancement and death rates. method necessitates exclusive gaming data that isn't available for historical market research and player segmentation instead of the actual games.

Multiple clustering models were compared by Sifa et al. \cite{22sifa2015user} (K-Means, Hierarchical approaches, DBSCAN) for user modeling in game analytics, proving the significance of algorithm selection significantly affects cluster interpretability,  discovery that inspires our comparison of multiple algorithms. But their work was concentrated solely on player engagement trends (duration of sessions,churn forecast) as opposed to the connection between crucial reception as well as business performance.

Commercial websites such as Steam and Metacritic \cite{2Cho} use cooperative filtering to suggest games, with an emphasis on matching user preferences on an individual basis instead of comprehending larger market niches. These systems are opaque in various methods of aggregation and consider every game to be an existing on a single scale of excellence, neglecting to acknowledge that games may achieve success via several means (critical acclaim vs. appeal to the general market).

The lack of unsupervised clustering applied directly to game-level variables (sales, critic scores, user scores) in order to identify quality archetypes is the gap in the existing literature.Neither (1) compared the effectiveness of several clustering techniques on game market data, (2) developed features to account for critic-user divergence, or (3) offered market divisions that were clear and comprehensible, going beyond basic rating aggregation.


In order to fill in these gaps, this study uses four cluster- K-Means, Hierarchical, DBSCAN, to 6,900 titles that have been fully sold and reviewed, with information employing three complementary criteria to compare their performance (Calinski-Harabasz, Davies-Bouldin Index, Silhouette score), as well as offering thorough descriptions of archetypes has critical and commercial dimensions. As opposed to player-centric Unlike recommendation systems or behavioral clustering, our method provides useful market intelligence that is available to independent researchers, publishers, and developers without needing exclusive telemetry information.


\section{Methodology}

This section describes the dataset, algorithms, tools, and techniques used in this study. The general framework follows a standard unsupervised learning pipeline: data collection, exploratory data analysis, data preprocessing (including feature engineering, log transformation, and dimensionality reduction via PCA), model training using three clustering algorithms (K-Means, DBSCAN, and Agglomerative Clustering), and evaluation using internal cluster validity metrics. The following subsections describe each stage in detail.

\subsection{Data Collection}

The dataset used in this study is the ``Video Game Sales with Ratings'' dataset published on Kaggle by Rush4Ratio \cite{rush4ratio2016videogamesales}. It was originally compiled by web-scraping VGChartz for sales data and Metacritic for review scores, covering video games released up to December 22, 2016. The raw dataset contains 16,719 records and 16 features, including game metadata (Name, Platform, Year of Release, Genre, Publisher, Developer, Rating), regional and global sales figures (NA Sales, EU Sales, JP Sales, Other Sales, Global Sales in millions of units), and review metrics (Critic Score, Critic Count, User Score, User Count).

The dataset was accessed programmatically from a public GitHub repository hosting the CSV file. No additional data collection was performed by the authors. The data was not collected by the authors; it was gathered by the original Kaggle contributor through automated scraping of publicly available sales charts and review aggregation platforms.

\subsection{Exploratory Data Analysis}

The dataset was examined to understand its structure, distributions, and relationships before applying any clustering algorithms. After loading the raw data, the following observations were made.

The dataset contains 16,719 samples and 16 features. Feature types include numerical (all sales columns, Critic Score, Critic Count, User Score, User Count) and categorical (Name, Platform, Year of Release, Genre, Publisher, Developer, Rating). The User Score column was stored as an object type due to the presence of ``tbd'' (to be determined) entries, which required conversion to numeric values.

Missing values were present across several features. Critic Score and Critic Count had a substantial proportion of missing entries, as did User Score and User Count. Year of Release, Publisher, Developer, and Rating also contained missing values. The exact missing value counts were printed and inspected during analysis.

Regional sales distributions (NA Sales, EU Sales, JP Sales, Other Sales) were examined using histograms. All four distributions are heavily right-skewed, with the majority of games selling below 1 million units and a small number of titles achieving very high sales. This skewness indicated the need for log transformation during preprocessing.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/regionalsalesdistribution.png}
\caption{Distribution of regional sales across North America, Europe, Japan, and other regions. All distributions exhibit strong right skew.}
\label{fig:regional_sales}
\end{figure}

Global Sales, Critic Score, and User Score distributions were plotted to understand the central tendencies. Critic Scores are roughly normally distributed with a slight left skew, centering around 70. User Scores center around 7.0 on the 0 to 10 scale. Global Sales follow the same right-skewed pattern as the regional figures.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/YEAH.png}
\caption{Distributions of Global Sales, Critic Score, and User Score showing the spread and central tendency of key clustering features.}
\label{fig:key_distributions}
\end{figure}

Scatter plots were constructed to explore the relationship between scores and sales. Critic Score versus Global Sales showed a weak positive trend, with higher-scored games tending to sell more, but with significant variance. User Score versus Global Sales showed a similar but weaker pattern. Critic Score versus User Score (scaled to 100) revealed moderate agreement between critics and users, though many games showed notable discrepancies.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/YEAH2.png}
\caption{Scatter plots showing relationships between Critic Score, User Score, and Global Sales.}
\label{fig:scatter_scores_sales}
\end{figure}

A User-Critic Discrepancy feature was engineered by scaling User Score to a 0 to 100 range and subtracting Critic Score. The distribution of this discrepancy was approximately normal and centered slightly below zero, indicating that critics tend to rate games slightly higher than users on average.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/6.png}
\caption{Distribution of User-Critic Discrepancy (User Score scaled to 100 minus Critic Score). The dashed red line marks perfect agreement.}
\label{fig:discrepancy}
\end{figure}

A correlation matrix was computed among Global Sales, Critic Score, User Score, Critic Count, and User Count. Critic Count and User Count showed relatively strong positive correlation with Global Sales, suggesting that more popular games attract more reviews. Critic Score and User Score showed moderate positive correlation with each other but weaker correlation with sales.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/7.png}
\caption{Correlation matrix heatmap of key numerical features. Critic Count and User Count show the strongest correlation with Global Sales.}
\label{fig:correlation}
\end{figure}

Boxplot analysis on Global Sales (plotted on a log scale due to extreme outliers) confirmed the presence of high-sales outliers. The 75th percentile and median were both relatively low compared to the maximum, reinforcing the skewed nature of the data. Review count distributions (Critic Count and User Count) were also right-skewed. A bar chart of average sales by region showed North America as the dominant market, followed by Europe, Japan, and other regions.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/8.png}
\caption{Boxplot of Global Sales (log scale), distributions of Critic Count and User Count, and average sales by region.}
\label{fig:outliers_regions}
\end{figure}

Duplicate records were checked using the Name column. Duplicate names were found and subsequently removed during data cleaning, retaining only the first occurrence.

No noisy or inconsistent labels apply to this study since it uses unsupervised learning and does not rely on labeled target variables.

Regarding redundant or highly correlated features, the regional sales columns (NA Sales, EU Sales, JP Sales, Other Sales) were found to contribute low additional variance when included alongside Global Sales. Since Global Sales is the sum of all regional sales, including regional breakdowns introduced multicollinearity. Therefore, only Global Sales was retained in the final feature set for clustering, and the individual regional sales columns were excluded.

\subsection{Data Preprocessing}

Data cleaning involved several steps. First, the User Score column was converted from object to numeric type, with non-numeric entries (such as ``tbd'') coerced to NaN. Second, games with fewer than 5 critic reviews (Critic Count less than 5) or fewer than 5 user reviews (User Count less than 5) were removed to ensure that scores were based on a minimally reliable sample size. Third, all rows with any missing values in the key columns (Name, NA Sales, EU Sales, JP Sales, Other Sales, Global Sales, Critic Score, Critic Count, User Score, User Count) were dropped. Fourth, duplicate records based on the Name column were removed.

Feature engineering produced two additional columns. User Score Scaled was computed by multiplying User Score by 10, placing it on the same 0 to 100 scale as Critic Score. User Critic Discrepancy was computed as User Score Scaled minus Critic Score, capturing whether users rated a game higher or lower than critics.

The final feature set used for clustering consisted of six variables: Global Sales, Critic Score, Critic Count, User Score Scaled, User Count, and User Critic Discrepancy.

To address the strong right skew observed in sales and count features, a log transformation ($\log(1 + x)$) was applied to all six features. Prior to transformation, the User Critic Discrepancy column was shifted to be entirely non-negative by subtracting its minimum value, since log transformation requires non-negative inputs.

Standard scaling was considered but ultimately replaced by log transformation, which more effectively compressed the range of heavily skewed features and reduced the influence of extreme outliers on the clustering algorithms.

Dimensionality reduction was performed using Principal Component Analysis (PCA). Both two-component and three-component reductions were evaluated. The three-component PCA retained a higher total explained variance compared to the two-component version, and produced better-defined clusters in visual inspection. Therefore, PCA with three components was selected, and all subsequent clustering was performed on the three-dimensional reduced feature space.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/YEAH4.png}
\caption{Comparison of individual explained variance per principal component for $n=2$ and $n=3$, and total explained variance for both configurations.}
\label{fig:pca_variance}
\end{figure}

PCA loading heatmaps were examined for both configurations. The loadings revealed which original features contributed most to each principal component, confirming that the components captured meaningful combinations of sales performance, review activity, and score-based features.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/YEAH3.png}
\caption{PCA loading heatmaps for $n=2$ and $n=3$ showing the contribution of each original feature to the principal components.}
\label{fig:pca_loadings}
\end{figure}

\subsection{Experimental Setup}

The following tools and frameworks were used. Python 3 served as the primary programming language. Pandas (data manipulation), NumPy (numerical operations), Matplotlib and Seaborn (static visualizations), Plotly (interactive 3D scatter plots), and Scikit-learn (preprocessing, PCA, clustering, evaluation metrics) were the core libraries. SciPy was used for hierarchical clustering linkage and dendrogram generation. The development environment was a Jupyter Notebook running in Visual Studio Code.

All experiments were conducted on a local machine. No GPU acceleration or cloud computing resources were required, as the dataset size and algorithmic complexity were manageable on standard hardware.

The experiments were organized as follows. After preprocessing and PCA, three clustering algorithms were applied to the same three-dimensional PCA-reduced dataset. Each algorithm was evaluated using the same set of internal validity metrics. Hyperparameter selection for each algorithm is described in the following subsections.

\subsection{Algorithm}

Three unsupervised clustering algorithms were selected for this study: K-Means, DBSCAN, and Agglomerative (Hierarchical) Clustering.

K-Means was chosen as the primary algorithm due to its simplicity, computational efficiency, and effectiveness on datasets with roughly spherical clusters. It partitions the data into $k$ clusters by minimizing the within-cluster sum of squares (WCSS). The k-means++ initialization strategy was used to improve convergence and avoid poor local minima.

DBSCAN (Density-Based Spatial Clustering of Applications with Noise) was selected as a density-based alternative that does not require specifying the number of clusters in advance and can identify noise points (outliers). This is particularly relevant for this dataset, which contains extreme sales outliers. DBSCAN groups together points that are closely packed and marks points in low-density regions as noise.

Agglomerative Clustering was chosen as a hierarchical approach that builds clusters by iteratively merging the closest pairs of clusters. The Ward linkage method was used, which minimizes the total within-cluster variance at each merge step. This algorithm provides a dendrogram visualization that helps understand the hierarchical structure of the data.

These three algorithms were selected to provide a comparison across fundamentally different clustering paradigms: centroid-based (K-Means), density-based (DBSCAN), and connectivity-based (Agglomerative). This diversity ensures a more robust assessment of the natural groupings in the data.

\subsection{Training Procedure}

For K-Means, the optimal number of clusters was determined through two methods. First, the Elbow Method was applied by computing WCSS for $k$ values ranging from 1 to 10 and identifying the ``elbow'' point where the rate of decrease in WCSS diminished. Second, Silhouette Score analysis was performed for $k$ values from 2 to 10 to find the number of clusters that maximized average silhouette width. Based on these analyses, $k=3$ was selected. The final K-Means model used k-means++ initialization, a maximum of 300 iterations, 10 independent runs ($n\_init=10$), and a random state of 42 for reproducibility.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/image14.png}
\caption{WCSS (Elbow Method) plot for K-Means showing the within-cluster sum of squares for $k=1$ through $k=10$.}
\label{fig:elbow}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/sdfsaa.png}
\caption{Silhouette Score analysis for K-Means across different values of $k$.}
\label{fig:silhouette}
\end{figure}

For DBSCAN, a k-distance graph was first generated using the 20 nearest neighbors to visually estimate a suitable $\varepsilon$ value range. A grid search was then conducted over $\varepsilon$ values ranging from 0.1 to 0.3 (step 0.01) and $min\_samples$ values from 3 to 4. For each parameter combination, the Silhouette Score was computed (only when valid clusters were formed). The combination yielding the highest Silhouette Score was selected as the optimal parameter set.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/image9.png}
\caption{K-Distance graph used to identify the appropriate $\varepsilon$ range for DBSCAN.}
\label{fig:kdistance}
\end{figure}

For Agglomerative Clustering, a dendrogram was generated using Ward linkage to visualize the merge distances and determine an appropriate number of clusters. Based on the dendrogram structure, $n\_clusters=3$ was chosen to match the K-Means configuration for fair comparison.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/image12.png}
\caption{Dendrogram of hierarchical clustering using Ward linkage, truncated to the last 30 merges.}
\label{fig:dendrogram}
\end{figure}

\subsection{Evaluation Metrics}

Three internal cluster validity metrics were used to evaluate and compare the clustering results.

Silhouette Score measures how similar each data point is to its own cluster compared to the nearest neighboring cluster. Values range from $-1$ to $1$, where higher values indicate better-defined, well-separated clusters. A score near 0 indicates overlapping clusters, and negative values suggest misclassified points. This metric was chosen because it balances both cluster cohesion and separation in a single measure.

Davies-Bouldin Index measures the average similarity ratio of each cluster with its most similar cluster, where similarity is defined as the ratio of within-cluster distances to between-cluster distances. Lower values indicate better clustering, with a score of 0 representing perfect clustering. This metric complements the Silhouette Score by providing an alternative perspective on cluster quality.

Calinski-Harabasz Index (also known as the Variance Ratio Criterion) measures the ratio of between-cluster dispersion to within-cluster dispersion. Higher values indicate denser, better-separated clusters. This metric was included because it is computationally efficient and provides a scale-dependent measure of cluster quality.

These three metrics were chosen because they are standard internal evaluation measures for unsupervised learning tasks where ground truth labels are unavailable. Together, they provide a comprehensive assessment of cluster quality from complementary perspectives: point-level (Silhouette), cluster-level similarity (Davies-Bouldin), and variance-based (Calinski-Harabasz).

For DBSCAN, noise points (labeled as $-1$) were excluded from metric computation to ensure fair comparison, as including unassigned noise points would artificially degrade the scores.

\subsection{Comparison of Clustering Algorithms}

The three clustering algorithms (K-Means, DBSCAN, and Agglomerative Clustering) were compared directly using the same evaluation metrics computed on the same PCA-reduced dataset.

In addition to comparing the three primary algorithms, two baseline methods were implemented to contextualize the results.

The first baseline was Random Assignment, where each data point was randomly assigned to one of three clusters. This serves as a lower-bound reference to confirm that the clustering algorithms discover meaningful structure beyond what would occur by chance.

The second baseline was Single-Feature Clustering, where K-Means ($k=3$) was applied using only the first principal component (which captures the most variance, primarily from Global Sales). This baseline tests whether the multi-dimensional feature space provides better segmentation than simply grouping games by sales volume alone.

The Silhouette Score was used as the primary comparison metric across all models and baselines. Improvement percentages were calculated relative to each baseline to quantify the added value of the full clustering pipeline.

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/image8.png}
\caption{Comparison table of Silhouette Score, Davies-Bouldin Index, and Calinski-Harabasz Index across K-Means, DBSCAN, and Agglomerative Clustering.}
\label{fig:model_comparison}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=\columnwidth]{imgchap3/9.png}
\caption{Baseline comparison of Silhouette Scores for the multi-dimensional K-Means model, single-feature clustering, and random assignment.}
\label{fig:baseline_comparison}
\end{figure}

Based on the evaluation metrics, K-Means achieved the most consistent balance of high silhouette score, low Davies-Bouldin index, and high Calinski-Harabasz index, and was therefore selected as the final model for cluster interpretation and archetype assignment.


\section{Results and Discussion}

\subsection{Cluster Discovery and Archetype Interpretation}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{imgchap4/wcss.png}
\caption{Elbow method for WCSS.}
\label{fig:pca}
\end{figure}

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{imgchap4/silhouette.png}
\caption{Elbow method and silhouette scores for determining optimal number of clusters.}
\label{fig:pca}
\end{figure}

Figures~\ref{fig:elbow} and~\ref{fig:silhouette} show the elbow method and silhouette scores for the preprocessed features. For the WCSS graph, even if 2 clusters seemingly provide a better score because it is at the elbow, we determined that 2 clusters would not give meaningful separation of clusters so we settled with \(k=3\). For the silhouette score, we can see that the elbow lines up well with \(k=3\). This helps justify our choice of picking 3 clusters for our K-Means and Hierarchical Clustering models.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{imgchap4/kmeans2.png}
\caption{ PCA projection of the dataset. Points are colour‑coded by K‑Means assignment. Three well‑separated groups are immediately visible, confirming the validity of the clustering.}
\label{fig:pca}
\end{figure}

Table~\ref{tab:cluster_centroids} lists the centroid values and defining traits of each cluster.

\begin{table}[htbp]
\centering\scriptsize
\caption{Interpretation for the three clusters.}
\label{tab:cluster_centroids}
\renewcommand\arraystretch{1.3}
\setlength{\tabcolsep}{2.7pt}
\begin{tabular}{|c|p{1.8cm}|p{1.4cm}|p{1.4cm}|p{1.4cm}|p{1.4cm}|}
\hline
\textbf{Label} & \textbf{Interpretation} & \textbf{Global \newline Sales (M)} & \textbf{Critic \newline Score} & \textbf{User Score \newline Scaled} & \textbf{User-Critic Δ} \\
\hline
0 & Mass Market Blockbusters & 2.941 & 82.458 & 76.488 & -5.970 \\
\hline
1 & Underrated Gems & 0.341 & 64.407 & 70.941 & 6.534 \\
\hline
2 & Commercial Titans & 0.971 & 73.618 & 75.703 & 2.085 \\
\hline
\end{tabular}
\end{table}

Key insight: Underrated Gems exhibit a positive user‑critic discrepancy of 6.534107 points, whereas the other clusters show near‑zero or negative differences. This empirically demonstrates that a subset of games achieves strong player appreciation despite lukewarm critical reception. A pattern invisible to simple score averaging.

\subsection{Quantitative Benchmarking and Model Selection}

We compared K‑Means against DBSCAN and Agglomerative Hierarchical clustering. Table~\ref{tab:algorithm_comparison} reports internal validation metrics.

\begin{table}[htbp]
\centering\scriptsize
\caption{Clustering algorithm performance comparison.}
\label{tab:algorithm_comparison}
\renewcommand\arraystretch{1.2}
\setlength{\tabcolsep}{2pt}
\begin{tabular}{|l|c|c|c|c|}
\hline
\textbf{Algorithm} & \textbf{Silhouette \(\uparrow\)} & \textbf{Davies‑Bouldin \(\downarrow\)} & \textbf{Calinski‑Harabasz \(\uparrow\)} & \textbf{Clusters} \\
\hline
K‑Means           & 0.407 & 0.848 & 5416.970 & 3          \\
\hline
DBSCAN            & 0.222 & 0.564 & 27.678   & 6          \\
\hline
Hierarchical      & 0.382 & 0.906 & 4278.104 & 3          \\
\hline
\end{tabular}
\end{table}

K‑Means outperforms alternatives on all three metrics. The silhouette score of 0.407307 indicates moderate‑to‑strong cluster cohesion.

\begin{figure}[htbp]
\centering
\includegraphics[width=0.7\linewidth]{imgchap4/dbscan.png}
\caption{Error analysis: DBSCAN labeled 139 games as noise (-1). This figure plots these noise points (dark blue) against the clustered data on the PCA axes. The visualization shows that noise points are primarily distributed on the periphery and in sparse regions above the main density clusters, indicating they lack sufficient local density to meet the \texttt{min\_samples} threshold.}
\label{fig:noise}
\end{figure}

\begin{table}[htbp]
\centering
\caption{Baseline comparisons: silhouette scores for different clustering approaches.}
\label{tab:baseline}
\setlength{\tabcolsep}{4pt} % optional: adjust spacing if needed
\begin{tabular}{|l|c|}
\hline
\textbf{Model} & \textbf{Silhouette Score} \\
\hline
K-Means (Multi-dimensional) & 0.407307 \\
\hline
Single-feature (Global Sales) & 0.408299 \\
\hline
Random Assignment & -0.006365 \\
\hline
\end{tabular}
\end{table}

Baseline comparisons: The multi-dimensional K-Means model achieves a silhouette score of 0.407, compared to 0.408 for clustering using Global Sales alone. The negligible difference indicates that Global Sales captures most of the inherent clustering structure in the dataset. However, both approaches significantly outperform random assignment (−0.006), confirming that meaningful clusters exist within the data.



\section{Conclusion}
This research focused on finding a method to classify video games into quality archetypes automatically by looking for the relationship between how well they sell and their review ratings. Traditional methods use one average rating to summarize all of the different aspects of quality, while in reality, quality is multi-dimensional in nature and very complex, making it difficult to determine how to classify games by quality \cite{1essentialfacts}, \cite{2Cho}. Our goal was to create an unsupervised framework that would allow us to identify the different types of game segments without having to use manual labels and provide a clear picture of all the different ways a game can be successful.

We applied three clustering algorithms (K-Means\cite{8macqueen1967multivariate} \cite{9arthur2007k}, Hierarchical Agglomerative \cite{10jain1988algorithms}, and DBSCAN \cite{12ester1996density}) to a dataset of 6,900 video games published between 1980 and 2016\cite{26Kirubi_2016}. We found three main quality archetypes Mass Market Blockbusters, Underrated Gems, Commercial
Titans using K-Means as it produced the most consistent balance of high separation and high density across the majority of metrics (Silhouette Score \cite{17rousseeuw1987silhouettes} = 0.447, Calinski Harabasz Score \cite{19calinski1974dendrite} = 1673.83). Importantly, we showed how being acknowledged by critics does not assure a successful game (e.g. Cluster 0 game sales were 63\% lower than those of Cluster 2 games). The work validated that there are different pathways where game publishers succeed in the video game market. We used these engineered features (User\_Critic\_Discrepancy, User\_Score\_Scaled) to identify different patterns of divergence between both types of ratings, which could not have been accomplished using a simple aggregate rating approach.

The main benefit of this research is comparative clustering method for evaluating the overall quality of video games that moves away from relying on player-based behavioral clusters \cite{21drachen2012guns}, \cite{22sifa2015user}, \cite{23bauckhage2014how}, which typically use proprietary telemetry data to assess purchase likelihood through supervised regression analysis, and provides instead a method to identify easily-interpreted archetypes of your product from publicly available data, thus opening up access to market evaluation opportunities for indie game developers and researchers. By providing independent developers with a means of developing market-value-based strategies to promote their titles (targeting critical prestige vs. mass-market appeal), publishers with a way to allocate their marketing budgets in a more effective manner, and consumers with a means of finding games of similar quality based on other attributes aside from numerical/historical scores, this framework opens up new avenues for indie game developers who are often in competition with larger publishers but lack the financial resources to establish their presence in the marketplace. Furthermore, the transparency of this framework (the identification of well-defined characteristics of each cluster, with the ability to rank the importance of features for each characteristic) addresses the common concern that commercial review/scoreboards do not provide enough market context to help customers make informed purchasing choices.

This study has limitations, which include: (1) a cutoff point for temporal coverage ending in 2016, so modern trends such as live-service games and early access models are missing; (2) there was a 59\% exclusion of data due to the absence of reviews \cite{rush4ratio2016videogamesales}, resulting in an inherent bias for games with solid documentation; (3) compounding due to bundling as evidenced by the predominance of hardware-bundled software in cluster II (i.e., Wii Sports); and (4) regional aggregations mask market heterogeneity across North America, Europe, and Japan. Future research should assess archetype stability using data from 2017-2024, use regionally-based clustering methods to identify regional consumer preferences, model the temporal transition from archetypes to predict the sales trajectories of new products from the early sales signals \cite{23bauckhage2014how}, and develop bundle-adjusted measures of sales volume to provide cleaner estimates of quality. Some of the questions that remain open are: whether archetypes can predict long-term franchise success; what the association of specific game mechanics to archetype membership is, and whether this framework for unsupervised segmentation can be generalized to other entertainment industries (movies, books, music).

Unsupervised quality segmentation is a scalable and adaptable method of analyzing how games are successful in an ever-evolving marketplace as the gaming industry continues to experience change due to new platforms, new business models, and new global audiences \cite{10jain1988algorithms}. Through the use of automated discovery of archetypes based on metadata analysis (showing that typically prestige, popularity, and profitability tended to be positively correlated \cite{3chiu2022critical} \cite{4exploringvalueofproductreview}; therefore, they should all show highly positive correlations), unsupervised learning has been established as one of the primary tools for creating strategy based on data within the game industry, allowing stakeholders to use a more nuanced approach to determining what defines value as opposed to relying solely on simplistic associations (good reviews = high sales).


% if have a single appendix:
%\appendix[Proof of the Zonklar Equations]
% or
%\appendix  % for no appendix heading
% do not use \section anymore after \appendix, only \section*
% is possibly needed

% use appendices with more than one appendix
% then use \section to start each appendix
% you must declare a \section before using any
% \subsection or using \label (\appendices by itself
% starts a section numbered zero.)
%


% \appendices
% \section{Proof of the First Zonklar Equation}
% Appendix one text goes here.

% you can choose not to have a title for an appendix
% if you want by leaving the argument blank
% \section{}
% Appendix two text goes here.


% use section* for acknowledgment
% \section*{Acknowledgment}


% The authors would like to thank...


% Can use something like this to put references on a page
% by themselves when using endfloat and the captionsoff option.
\ifCLASSOPTIONcaptionsoff
  \newpage
\fi



% trigger a \newpage just before the given reference
% number - used to balance the columns on the last page
% adjust value as needed - may need to be readjusted if
% the document is modified later
%\IEEEtriggeratref{8}
% The "triggered" command can be changed if desired:
%\IEEEtriggercmd{\enlargethispage{-5in}}

% references section

% can use a bibliography generated by BibTeX as a .bbl file
% BibTeX documentation can be easily obtained at:
% http://mirror.ctan.org/biblio/bibtex/contrib/doc/
% The IEEEtran BibTeX style support page is at:
% http://www.michaelshell.org/tex/ieeetran/bibtex/
%\bibliographystyle{IEEEtran}
% argument is your BibTeX string definitions and bibliography database(s)
%\bibliography{IEEEabrv,../bib/paper}
%
% <OR> manually copy in the resultant .bbl file
% set second argument of \begin to the number of references
% (used to reserve space for the reference number labels box)
% \begin{thebibliography}{1}f

\bibliography{bibfile.bib}{}
\bibliographystyle{IEEEtran}

% biography section
% 
% If you have an EPS/PDF photo (graphicx package needed) extra braces are
% needed around the contents of the optional argument to biography to prevent
% the LaTeX parser from getting confused when it sees the complicated
% \includegraphics command within an optional argument. (You could create
% your own custom macro containing the \includegraphics command to make things
% simpler here.)
%\begin{IEEEbiography}[{\includegraphics[width=1in,height=1.25in,clip,keepaspectratio]{mshell}}]{Michael Shell}
% or if you just want to reserve a space for a photo:

% \begin{IEEEbiography}{Michael Shell}
% Biography text here.
% \end{IEEEbiography}

% if you will not have a photo at all:
% \begin{IEEEbiographynophoto}{John Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% insert where needed to balance the two columns on the last page with
% biographies
%\newpage

% \begin{IEEEbiographynophoto}{Jane Doe}
% Biography text here.
% \end{IEEEbiographynophoto}

% You can push biographies down or up by placing
% a \vfill before or after them. The appropriate
% use of \vfill depends on what kind of text is
% on the last page and whether or not the columns
% are being equalized.

%\vfill

% Can be used to pull up biographies so that the bottom of the last one
% is flush with the other column.
%\enlargethispage{-5in}



% that's all folks
\end{document}


